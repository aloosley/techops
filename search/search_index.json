{"config":{"indexing":"full","lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 TechOps Introduction \u00b6 Are you looking for guidance on how to create technical documentation for your AI/ML system and/or its constituent models and datasets? Should your AI/ML system documentation provide value to a wide variety of user personas (managers, system users, developers, researchers, regulators, etc.)? Does your AI/ML system need to comply with the EU AI Act, or are you developing a model or dataset that you want to be used as part of an AI/ML system that needs to comply with the EU AI Act? If you answered yes to any of these questions, TechOps is for you. TechOps, published in AIES 2025 (see citation details below), is a set of documentation templates and examples designed to help technical teams and companies document their AI/ML applications, models, and datasets. TechOps is also the first set of documentation templates that we know of to completely map its sections to the EU AI Act, for developers and providers that need to comply with this regulation that first came into force in August 2024. While the primary contribution of TechOps is documentation guidance and examples, we also recognize that not every user already has an approach to rendering their documentation to maximize its value in the real world to a wide variety of personas (managers, system users, developers, researchers, regulators, etc.). Therefore, we also provide a [blueprint for rendering your own TechOps Documentation]. The TechOps documentation examples seen on this very website are rendered using this blueprint. See Getting Started below for more details. Citing \u00b6 If you use and parts of this work, we kindly ask that you cite our paper . Templates \u00b6 TechOps are three separate templates for sufficiently documenting AI systems for proof of compliance with the AI Act. The documentation is split into three levels: AI System documentation Model documentation Data documentation to allow the owners of data, models, and AI systems to each maintain ownership of their own level of documentation. Thus, model and dataset owners whom may or may not have curated their models and datasets with a specific AI Systems in mind, may still create documentation contributions that the AI System documentation can reference. These templates are meant to guide responsible stakeholders to document AI systems across various fields. Unlike existing lengthy and abstract questionnaires, these templates offer clear guidance for the documentation of the relevant processes across the AI lifecycle, translating complex requirements such as fairness and data governance into actionable metrics and measurable criteria that can be implemented and tracked. This process ensures that the abstract legal requirements of the AI Act are operationalized into concrete actions, making them manageable and measurable. Following the TechOps approach also provides stakeholders comprehensive oversight on the data, model and application lifecycle. These templates track the system\u2019s status over the entire AI lifecycle, ensuring traceability, reproducibility, in addition to compliance with the AI Act. Clear documentation also promotes discoverability, collaboration, and risk assessment. Examples \u00b6 The templates are tested on real-world scenarios providing examples that further guide their implementation: Description Application Documentation Example SafeSiteAI is a fictional high-risk AI system developed to detect and monitor construction worker safety using real-time video analytics and sensor fusion. Model Documentation Example A neural network for segmenting human silhouettes in photos Data Documentation Example A skin tones dataset created to support fairness evaluations of downstream computer vision models and human centric applications like SafeSiteAI Getting Started \u00b6 Developing TechOps documentation is easy, it's just markdown! Simply create your own markdown files following the TechOps Documentation Templates (For template files, see our GitHub Page ). How you render your TechOps Documentation is up to you, but we also provide a blueprinted approach for rendering TechOps documentation. To use it, follow the instructions on our GitHub Page .","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#techops-introduction","text":"Are you looking for guidance on how to create technical documentation for your AI/ML system and/or its constituent models and datasets? Should your AI/ML system documentation provide value to a wide variety of user personas (managers, system users, developers, researchers, regulators, etc.)? Does your AI/ML system need to comply with the EU AI Act, or are you developing a model or dataset that you want to be used as part of an AI/ML system that needs to comply with the EU AI Act? If you answered yes to any of these questions, TechOps is for you. TechOps, published in AIES 2025 (see citation details below), is a set of documentation templates and examples designed to help technical teams and companies document their AI/ML applications, models, and datasets. TechOps is also the first set of documentation templates that we know of to completely map its sections to the EU AI Act, for developers and providers that need to comply with this regulation that first came into force in August 2024. While the primary contribution of TechOps is documentation guidance and examples, we also recognize that not every user already has an approach to rendering their documentation to maximize its value in the real world to a wide variety of personas (managers, system users, developers, researchers, regulators, etc.). Therefore, we also provide a [blueprint for rendering your own TechOps Documentation]. The TechOps documentation examples seen on this very website are rendered using this blueprint. See Getting Started below for more details.","title":"TechOps Introduction"},{"location":"#citing","text":"If you use and parts of this work, we kindly ask that you cite our paper .","title":"Citing"},{"location":"#templates","text":"TechOps are three separate templates for sufficiently documenting AI systems for proof of compliance with the AI Act. The documentation is split into three levels: AI System documentation Model documentation Data documentation to allow the owners of data, models, and AI systems to each maintain ownership of their own level of documentation. Thus, model and dataset owners whom may or may not have curated their models and datasets with a specific AI Systems in mind, may still create documentation contributions that the AI System documentation can reference. These templates are meant to guide responsible stakeholders to document AI systems across various fields. Unlike existing lengthy and abstract questionnaires, these templates offer clear guidance for the documentation of the relevant processes across the AI lifecycle, translating complex requirements such as fairness and data governance into actionable metrics and measurable criteria that can be implemented and tracked. This process ensures that the abstract legal requirements of the AI Act are operationalized into concrete actions, making them manageable and measurable. Following the TechOps approach also provides stakeholders comprehensive oversight on the data, model and application lifecycle. These templates track the system\u2019s status over the entire AI lifecycle, ensuring traceability, reproducibility, in addition to compliance with the AI Act. Clear documentation also promotes discoverability, collaboration, and risk assessment.","title":"Templates"},{"location":"#examples","text":"The templates are tested on real-world scenarios providing examples that further guide their implementation: Description Application Documentation Example SafeSiteAI is a fictional high-risk AI system developed to detect and monitor construction worker safety using real-time video analytics and sensor fusion. Model Documentation Example A neural network for segmenting human silhouettes in photos Data Documentation Example A skin tones dataset created to support fairness evaluations of downstream computer vision models and human centric applications like SafeSiteAI","title":"Examples"},{"location":"#getting-started","text":"Developing TechOps documentation is easy, it's just markdown! Simply create your own markdown files following the TechOps Documentation Templates (For template files, see our GitHub Page ). How you render your TechOps Documentation is up to you, but we also provide a blueprinted approach for rendering TechOps documentation. To use it, follow the instructions on our GitHub Page .","title":"Getting Started"},{"location":"example-application-documentation-safesite-ai/","text":"SafeSiteAI Application Documentation \u00b6 Application Owner : George Costanza , Vandelay Industries Document Version : 0.4.1 Reviewers : Jerry Seinfeld , Alex Loosley Relevant Links \u00b6 GitHub Repository Deployment Pipeline API ( Swagger Docs ) AWS Account JIRA Backlog LeanIX Page General Information \u00b6 Purpose and Intended Use : SafeSiteAI is a fictional high-risk AI system developed to detect and monitor construction worker safety using real-time video analytics and sensor fusion. The system's primary objective is to identify unsafe behaviors (e.g., not wearing PPE, entering danger zones) and hazardous conditions (e.g., falling debris, proximity to heavy machinery) to trigger alerts, prevent accidents, and support regulatory compliance. Sector : Construction and Occupational Safety Problem Solved : Reduces on-site accidents, improves compliance with safety protocols Target Users : Site supervisors, safety managers, construction companies KPIs : Reduction in incidents, response time to alerts, detection accuracy Ethical/Regulatory : Adheres to GDPR for video data, complies with EU AI Act for high-risk systems Prohibited Uses : Employee surveillance beyond safety (e.g., productivity scoring) Operational Environment : Deployed on-site with edge devices, synced with a cloud dashboard Risk Classification \u00b6 Classification : High Reasoning : Monitors safety-critical scenarios in a workplace where human health is at risk, regulated under AI Act Article 6(1)(a) and (b) Application Functionality \u00b6 Instructions for Use for Deployers : System requires regular calibration and access to camera feeds and IoT sensors. Alerts are issued via on-site monitors and mobile apps. Model Capabilities : Detects PPE (helmets, vests), boundary breaches, fall incidents, and idle heavy machinery Limitations: Lower accuracy under poor lighting/weather conditions Languages supported: English, German Input Data Requirements : Real-time video feeds (HD cameras), sensor logs Valid inputs: stable camera angle, active worker zones Invalid inputs: occluded views, uncalibrated sensors Output Explanation : Generates classification alerts (e.g., PPE violation) with confidence scores Provides bounding box visualizations and timestamps System Architecture Overview : See SafeSiteAI LeanIX Architecture Models and Datasets \u00b6 Models \u00b6 Model Documentation Description of Application Usage ALiSNet TechOps Model Document Used for identity masked human and PPE segmentation PPE Fit and Usage Classifier TechOps Model Document * Used to score if PPE properly used Other Danger Classifier GitHub Repo * Used to detect other safety hazards in view Fictional example (no link provided) Datasets \u00b6 Dataset Documentation Description of Application Usage Skin tones dataset TechOps Data Document Used for end-to-end application testing PPE annotation data GitHub Repo * Used for end-to-end application testing Fictional example (no link provided) Note, other datasets were used to train, evaluate, and test models used as part of SafeSiteAI. See those documentation for details about those datasets. Application Development \u00b6 Deployment \u00b6 Cloud Setup : See LeanIX Page APIs : Backend (see Swagger Documentation ) Integration : Application integrated into Vandelay Industries SafeSiteAI Suite Integration with External Systems \u00b6 Systems : Dependencies: Camera hardware, sensor suite, on-site edge gateway Data flow diagrams showing interactions: Site -> Edge Device -> Cloud -> Dashboard Error-handling mechanisms: Retry logic, manual fallback Deployment Plan \u00b6 Infrastructure : Environments: development, staging, production Scaling: Auto-scale based on number of cameras Backup and recovery: Daily snapshots, recovery under 15 minutes Integration Steps : Deploy pipeline: edge device \u2192 data store \u2192 alert engine \u2192 dashboard Dependencies: PyTorch, OpenCV, AWS SDK Rollback: Last-stable container with health-check triggers User Information : Currently deployed across 6 EU construction sites Lifecycle Management \u00b6 Monitoring : Performance: Detection latency, accuracy (see Grafana monitoring dashboard ) Infrastructure: CPU, memory, network (see Grafana monitoring dashboard ) Versioning and change logs : See Application GitHub Repository Key Activities : Continuous evaluation Retraining quarterly or on demand Documentation Needs : Logs: Accuracy drift Audit trails: Model update logs Incident reports: Alert false positives/negatives Risk Management System \u00b6 Methodology : ISO 31000 + Hazard Identification Matrix Identified Risks : False negatives (undetected hazards) Privacy violations (video storage) Mitigation : Preventive: Diverse evaluation data , identity masking (ALiSNet) Protective: Manual override, encrypted alert logs Testing and Validation (Accuracy, Robustness, Cybersecurity) \u00b6 Accuracy : See latest deployed model scores in Model Registry Robustness : See latest deployed model scores in Model Registry Cybersecurity : TLS 1.3, IAM roles, intrusion logging See Monitoring Dashboard Human Oversight \u00b6 Human-in-the-loop : Required for alert validation Override : Site manager may suppress or escalate alerts Training : Safety staff trained on alert review interface Limitations : Cannot detect underground/chemical hazards - See QA Board for more details Incident Management \u00b6 Common Issues : False alarms from shadows/motion Compatibility issues with legacy hardware Support Contact : 24/7 tech hotline , escalation team EU Declaration of Conformity \u00b6 System Name : SafeSiteAI Provider : Vandelay Industries, New York, NY, 10101 USA Compliance : EU AI Act Art. 47, GDPR, ISO/IEC 27001 Standards Applied \u00b6 ISO 31000: Risk Management ISO/IEC 27001: Information Security IEC 61508: Functional Safety See Governance Approvals for more information Documentation Metadata \u00b6 Template Version \u00b6 Link to Template Documentation Authors \u00b6 George Costanza , Importer/Exporter: (Principal Developer)","title":"SafeSiteAI Application Documentation"},{"location":"example-application-documentation-safesite-ai/#safesiteai-application-documentation","text":"Application Owner : George Costanza , Vandelay Industries Document Version : 0.4.1 Reviewers : Jerry Seinfeld , Alex Loosley","title":"SafeSiteAI Application Documentation"},{"location":"example-application-documentation-safesite-ai/#relevant-links","text":"GitHub Repository Deployment Pipeline API ( Swagger Docs ) AWS Account JIRA Backlog LeanIX Page","title":"Relevant Links"},{"location":"example-application-documentation-safesite-ai/#general-information","text":"Purpose and Intended Use : SafeSiteAI is a fictional high-risk AI system developed to detect and monitor construction worker safety using real-time video analytics and sensor fusion. The system's primary objective is to identify unsafe behaviors (e.g., not wearing PPE, entering danger zones) and hazardous conditions (e.g., falling debris, proximity to heavy machinery) to trigger alerts, prevent accidents, and support regulatory compliance. Sector : Construction and Occupational Safety Problem Solved : Reduces on-site accidents, improves compliance with safety protocols Target Users : Site supervisors, safety managers, construction companies KPIs : Reduction in incidents, response time to alerts, detection accuracy Ethical/Regulatory : Adheres to GDPR for video data, complies with EU AI Act for high-risk systems Prohibited Uses : Employee surveillance beyond safety (e.g., productivity scoring) Operational Environment : Deployed on-site with edge devices, synced with a cloud dashboard","title":"General Information"},{"location":"example-application-documentation-safesite-ai/#risk-classification","text":"Classification : High Reasoning : Monitors safety-critical scenarios in a workplace where human health is at risk, regulated under AI Act Article 6(1)(a) and (b)","title":"Risk Classification"},{"location":"example-application-documentation-safesite-ai/#application-functionality","text":"Instructions for Use for Deployers : System requires regular calibration and access to camera feeds and IoT sensors. Alerts are issued via on-site monitors and mobile apps. Model Capabilities : Detects PPE (helmets, vests), boundary breaches, fall incidents, and idle heavy machinery Limitations: Lower accuracy under poor lighting/weather conditions Languages supported: English, German Input Data Requirements : Real-time video feeds (HD cameras), sensor logs Valid inputs: stable camera angle, active worker zones Invalid inputs: occluded views, uncalibrated sensors Output Explanation : Generates classification alerts (e.g., PPE violation) with confidence scores Provides bounding box visualizations and timestamps System Architecture Overview : See SafeSiteAI LeanIX Architecture","title":"Application Functionality"},{"location":"example-application-documentation-safesite-ai/#models-and-datasets","text":"","title":"Models and Datasets"},{"location":"example-application-documentation-safesite-ai/#models","text":"Model Documentation Description of Application Usage ALiSNet TechOps Model Document Used for identity masked human and PPE segmentation PPE Fit and Usage Classifier TechOps Model Document * Used to score if PPE properly used Other Danger Classifier GitHub Repo * Used to detect other safety hazards in view Fictional example (no link provided)","title":"Models"},{"location":"example-application-documentation-safesite-ai/#datasets","text":"Dataset Documentation Description of Application Usage Skin tones dataset TechOps Data Document Used for end-to-end application testing PPE annotation data GitHub Repo * Used for end-to-end application testing Fictional example (no link provided) Note, other datasets were used to train, evaluate, and test models used as part of SafeSiteAI. See those documentation for details about those datasets.","title":"Datasets"},{"location":"example-application-documentation-safesite-ai/#application-development","text":"","title":"Application Development"},{"location":"example-application-documentation-safesite-ai/#deployment","text":"Cloud Setup : See LeanIX Page APIs : Backend (see Swagger Documentation ) Integration : Application integrated into Vandelay Industries SafeSiteAI Suite","title":"Deployment"},{"location":"example-application-documentation-safesite-ai/#integration-with-external-systems","text":"Systems : Dependencies: Camera hardware, sensor suite, on-site edge gateway Data flow diagrams showing interactions: Site -> Edge Device -> Cloud -> Dashboard Error-handling mechanisms: Retry logic, manual fallback","title":"Integration with External Systems"},{"location":"example-application-documentation-safesite-ai/#deployment-plan","text":"Infrastructure : Environments: development, staging, production Scaling: Auto-scale based on number of cameras Backup and recovery: Daily snapshots, recovery under 15 minutes Integration Steps : Deploy pipeline: edge device \u2192 data store \u2192 alert engine \u2192 dashboard Dependencies: PyTorch, OpenCV, AWS SDK Rollback: Last-stable container with health-check triggers User Information : Currently deployed across 6 EU construction sites","title":"Deployment Plan"},{"location":"example-application-documentation-safesite-ai/#lifecycle-management","text":"Monitoring : Performance: Detection latency, accuracy (see Grafana monitoring dashboard ) Infrastructure: CPU, memory, network (see Grafana monitoring dashboard ) Versioning and change logs : See Application GitHub Repository Key Activities : Continuous evaluation Retraining quarterly or on demand Documentation Needs : Logs: Accuracy drift Audit trails: Model update logs Incident reports: Alert false positives/negatives","title":"Lifecycle Management"},{"location":"example-application-documentation-safesite-ai/#risk-management-system","text":"Methodology : ISO 31000 + Hazard Identification Matrix Identified Risks : False negatives (undetected hazards) Privacy violations (video storage) Mitigation : Preventive: Diverse evaluation data , identity masking (ALiSNet) Protective: Manual override, encrypted alert logs","title":"Risk Management System"},{"location":"example-application-documentation-safesite-ai/#testing-and-validation-accuracy-robustness-cybersecurity","text":"Accuracy : See latest deployed model scores in Model Registry Robustness : See latest deployed model scores in Model Registry Cybersecurity : TLS 1.3, IAM roles, intrusion logging See Monitoring Dashboard","title":"Testing and Validation (Accuracy, Robustness, Cybersecurity)"},{"location":"example-application-documentation-safesite-ai/#human-oversight","text":"Human-in-the-loop : Required for alert validation Override : Site manager may suppress or escalate alerts Training : Safety staff trained on alert review interface Limitations : Cannot detect underground/chemical hazards - See QA Board for more details","title":"Human Oversight"},{"location":"example-application-documentation-safesite-ai/#incident-management","text":"Common Issues : False alarms from shadows/motion Compatibility issues with legacy hardware Support Contact : 24/7 tech hotline , escalation team","title":"Incident Management"},{"location":"example-application-documentation-safesite-ai/#eu-declaration-of-conformity","text":"System Name : SafeSiteAI Provider : Vandelay Industries, New York, NY, 10101 USA Compliance : EU AI Act Art. 47, GDPR, ISO/IEC 27001","title":"EU Declaration of Conformity"},{"location":"example-application-documentation-safesite-ai/#standards-applied","text":"ISO 31000: Risk Management ISO/IEC 27001: Information Security IEC 61508: Functional Safety See Governance Approvals for more information","title":"Standards Applied"},{"location":"example-application-documentation-safesite-ai/#documentation-metadata","text":"","title":"Documentation Metadata"},{"location":"example-application-documentation-safesite-ai/#template-version","text":"Link to Template","title":"Template Version"},{"location":"example-application-documentation-safesite-ai/#documentation-authors","text":"George Costanza , Importer/Exporter: (Principal Developer)","title":"Documentation Authors"},{"location":"example-data-documentation-voc-skin-tones/","text":"Skin Tones Data Documentation \u00b6 Dataset Owner : George Costanza Document Version : 0.5.0 Reviewers : Jerry Seinfeld Data documentation contributions and feedback are welcome! Overview \u00b6 Primary Data Modality \u00b6 Image Data Tabular Data Dataset Description \u00b6 Post hoc skin tone labels of the faces of customers in the Zalando Voice of Customer (VOC) dataset curated as part of the \"Skin Tone Labeling Initiative.\" The primary purpose of this data is for fairness evaluation purposes: to help ensure data used to train ML/AI systems for Size and Fit is representative of Zalando's customers to ensure ML/AI systems do not systematically underperform for customers with certain skin tones (see Known Dataset Usage - Known Dataset Usage for more information about known fairness evaluations implemented using this dataset). This data was collected by a team of four Zalando labelers from a mix of teams including Beauty, Size and Fit, and Algorithmic Privacy and Fairness. Labelers followed specific Skin Tone Labeling Instructions , and labeled each image for skin tone based on the 2022 Zalando Beauty skin tone scale (shown below). Further technical details can be found in the Skin Tone Labeling GitHub Repository . Special thanks to the labelers: Alex Loosley , Algorithmic Privacy and Fairness Susan Ross , Algorithmic Privacy and Fairness Amrollah Seifoddini , Size and Fit Helen Seinfeld , Beauty Beyond fairness evaluations, this dataset along with the entire initiative has inpired the creation of a Zalando Skin Tone Labeling Playbook. Status \u00b6 Status Date: 27/04/2023 Under Preparation - The dataset is still under active curation and is not yet ready for use due to active \"dev\" updates. The dataset should be ready for wider usage in June 2023. Relevant Links \u00b6 Zalando VOC Skin Tones Dataset (S3 Bucket requiring access permissions) Zalando VOC Images Data Documentation (Base dataset used for labeling) Skin Tone Labeling Initiative Instructions for Labelers Dataset processing and analysis (GitHub Repository) Developers \u00b6 Alex Loosley , Algorithmic Privacy and Fairness: (Principal Developer) Amrollah Seifoddini , Size and Fit: (Dataset Owner) Owner \u00b6 Main point of contact: George Steinbrenner Team: Size and Fit (Zurich) Affiliation: Zalando SE Team Website: N/A Data Subject(s) \u00b6 Images of consenting customers Sensitive Data about people Skin tones labels Data Point Description \u00b6 A data point is made up of an image of a person, and one or more skin tone labels as defined in this Instructions for Labelers document. See the Examples of Data Points section for examples. Dataset Statistics \u00b6 Category Data Size of Dataset 1009 MB Number of Data Points 59 calibration + 999 main Label Classes 6 (5 skin tones, 1 for uncertainty) Type of labels Multiple labels per data point Algorithmic Labels 0 Human Labels All Additional Notes: The definitions of main and calibration splits and information on labels can be found in the Human Annotations section. Tables and Fields \u00b6 TABLE: labels_per_image \u00b6 Primary Key: annotation_image_id Description: Skin tone label data for each image. Field Name Type Description annotation_image_id str Primary key identifying image with respect to annotation job labels list[list[str]] List of multiple labels (inner lists) given by each labeler (outer list) skin_tone_values list[float] Skin tone values for each labeler (based on ZBeautySkinToneLabelEncoder ) valid_skin_tone_values list[float] Same as skin tone values with invalid value floats removed skin_tone_mean float Mean valid skin tone value skin_tone_std float Standard deviation of valid skin tone values Statistic skin_tone_mean count 999 mean 0.670921 std 0.629619 min 0. 25% 0.25 50% 0.5 75% 0.75 max 4. mode 0.5 Histogram of skin_tone_mean values: Additional Notes: Skin tone related labels and values are protected attributes, see the sensitive and protected attributes section for more details. TABLE: labeling_job_manifest \u00b6 Primary Key: image_id Description: Annotation job manifest, containing information about what was annotated. Field Name Type Description image_id str Unique image id annotation_image_id str image id given corresponding annotation job source-ref str s3 bucket location TABLE: data_per_label \u00b6 Primary Key: None Description: Information about each label, such as a labeler UUID and meta data like how much time was needed to produce the label. Field Name Type Description annotation_image_id str links to image_id in labels_per_image table workerId str unique id of worker (labeler) who labeled the image timeSpentInSeconds float time needed for labeler to label the image Dataset Version and Maintenance \u00b6 Version Details \u00b6 Current Data Version: Not currently tracked Data Version Release Date: 03/03/2023 Data Version for last Data Card Update: N/A Last Data Card Update: 10/03/2023 Data Change Log \u00b6 TBD Maintenance Plan \u00b6 This dataset is in development mode and not yet being maintained for usage by others. Versioning: TBD Updates: TBD Errors: TBD Feedback: TBD Next Planned Update(s) \u00b6 Version affected: Not currently tracked Next data update: Ongoing until version 1.0.0 Next version: 1.0.0 Next version update: 04/2023 Expected Change(s) \u00b6 Version 1.0.0 will be released once data curation and preparation is complete. Example of Data Points \u00b6 Typical Data Point \u00b6 A typical annotation example from the labels_per_image table: {\"annotation_image_id\": \"10\", \"skin_tone_values\": [3.5, 3.5, 3.0, 3.0], \"labels\": [[\"mid-deep\", \"deep\"], [\"mid-deep\", \"deep\"], [\"mid-deep\"], [\"mid-deep\"]], \"uncertain_labels\": [false, false, false, false], \"valid_skin_tone_values\": [3.5, 3.5, 3.0, 3.0], \"any_uncertainty\": false, \"complete_consensus\": false, \"overlap_consensus\": true, \"near_consensus\": true, \"n_skin_tone_values\": 4, \"frac_unique_values\": 0.5, \"n_unique_skin_tone_values\": 2, \"n_connected_skin_tone_groups\": 1, \"skin_tone_mean\": 3.25, \"skin_tone_std\": 0.25, \"has_invalid_annotation\": false, \"skin_tone_mode_value\": 3.0, \"skin_tone_mode_count\": 2, \"n_valid_labels\": 4} Here, annotators were not uncertain in their labels, two believed the skin tone was a combination of mid-deep and deep , and two thought the skin tone was just deep (note neighbouring skin-tone labels like mid-deep and deep were allowed, see Annotations and Labling for more details). Note, this data corresponds to the main labeling task which only had three labelers. See Annotations and Labeling for more details about labeling tasks. Atypical Data Point \u00b6 An atypical annotation example from the labels_per_image table where the first labeler seemed to have made a mistake by labeled a skin tone as both light and deep : {\"annotation_image_id\": \"24\", \"skin_tone_values\": [-1.0, 1.5, 0.0], \"labels\": [[\"light\", \"deep\"], [\"mid-light\", \"medium\"], [\"light\"]], \"uncertain_labels\": [false, false, false], \"valid_skin_tone_values\": [1.5, 0.0], \"any_uncertainty\": false, \"complete_consensus\": false, \"overlap_consensus\": false, \"near_consensus\": false, \"n_skin_tone_values\": 2, \"frac_unique_values\": 1.0, \"n_unique_skin_tone_values\": 2, \"n_connected_skin_tone_groups\": 2, \"skin_tone_mean\": 0.75, \"skin_tone_std\": 0.75, \"has_invalid_annotation\": true, \"skin_tone_mode_value\": 0.0, \"skin_tone_mode_count\": 1} Note, this data corresponds to the calibration 1 labeling task which had more labeling mistakes as labelers got used to the labeling UI. See Annotations and Labeling for more details about labeling tasks. Sampling of Data Points \u00b6 Example Type annotation_image_id Apparent skin tone human labelers agreed on labels 10 lighter 15 deeper human labelers were uncertain about label 14 lighter 255 deeper Additional Note : No actual images can be shown here because they require access approval (see Access section ). Purpose and Motivations \u00b6 Intended Purpose(s) \u00b6 Fairness Evaluation Motivating Factor(s) \u00b6 Assessing and publishing the distribution of skin tones in the Zalando VOC dataset Identifying potential sample bias in data that may be used for training computer vision systems at Zalando Providing a skin tone dataset for fairness evaluation Writing a skin tone labeling playbook for others who want to curate skin tones via post hoc human labeling See the skin-tone-labeling repository for more details. Intended Use \u00b6 Dataset Use(s) \u00b6 Skin tone fairness evalaution for pre-production models Suitable Use Case(s) \u00b6 Suitable Use Case: Use to evaluate (un)fairness of any model that should perform well for Zalando VOC type images of humans. For example, Zalando's Body Measurements Pipeline . Unsuitable Use Case(s) \u00b6 This data is, in its current form, not vetted for training a skin tone classifier that could be used at scale. Research and Problem Space(s) \u00b6 Skin tone fairness evaluation Analysis of bias in human skin tone annotations Information for Usage \u00b6 Usage Guideline(s) \u00b6 Usage Guidelines: This dataset is meant for fairness evaluation purposes only to ensure that models trained on the Zalando VOC dataset, or similar, do not systematically underperform for subjects with certain skin tones. Approval Steps: The reason of using this dataset for a particular use case must be described and approved via a DPR process . New DPRs should refer to this existing DPR , which pertains to the creation of this dataset. See the Accesss Prerequesites section. Reviewer: Please tag the data owner when creating a DPR. Use with Other Data \u00b6 Safety Level \u00b6 Safe to use with other data for fairness evaluation purposes Best Practices \u00b6 If presenting examples of this data is a must, consider blurring faces and backgrounds. Additional Notes: Add here Forking and Sampling \u00b6 Safety Level \u00b6 Conditionally safe to fork and/or sample Acceptable Sampling Method(s) \u00b6 Cluster Sampling Multi-stage sampling Stratified Sampling Unsampled Best Practice(s) \u00b6 This dataset is meant for fairness assessments against skin tone. Any samples should ensure that all skin tones are represented. Risk(s) and Mitigation(s) \u00b6 Summarize here. Include links and metrics where applicable. Unrepresenting skin tone groups: Sampling incorrectly risks certain skin tone groups being underrepresented for skin tone based fairness evaluations. Ensure all skin tones are well represented such as to have enough data points to estimate performance on particular skin tones with a low enough level of uncertainty to be able to draw reliable fairness conclusions. Notable Feature(s) \u00b6 Exploration Demo: Found in Jupyter Notebook Distribution(s) \u00b6 N/A - The entire main split of the dataset can be used for fairness evaluation. At this time, we do not recommend training models with this data, and therefore, do not have a recommended train-validation-test split. Known Correlation(s) \u00b6 None known at this time Split Statistics \u00b6 TBD Citation Guidelines \u00b6 Guidelines: Refer to this dataset by it's title and provide a reference link to this data card. Known Usage \u00b6 Models(s) \u00b6 Model Model Task Purpose of Dataset Usage AI Act Risk Size and Fit - On Device Silhouette Extraction image segmentation Fairness Evaluation Limited Note, this table may not be exhaustive. Dataset users and documentation consumers at large are highly encouraged to contribute known usages. Application(s) \u00b6 Application Brief Description Purpose of Dataset Usage AI Act Risk Size and Fit - Body Measurements Pipeline Pipeline from image of customer to body measurements including image segmentation and body reconstruction Fairnesse Evaluation Limited Size and Fit - Body Measurements Pipeline - 2022 Proof Of Concept An initial proof of concept to determine a best approach to doing fairness assessments on Size and Fit's body measurements pipeline Fairnesse Evaluation Limited Note, this table may not be exhaustive. Dataset users and documentation consumers at large are highly encouraged to contribute known usages. Access, Retention, and Deletion \u00b6 Access \u00b6 Relevant Links \u00b6 Zalando VOC Skin Tones Dataset (S3 Bucket requiring access permissions) Data Security Classification \u00b6 Yellow Prerequisite(s) \u00b6 Users requiring access must get approval on a DPR (with corresponding use case) either by: adding their user to the existing DPR creating a new DPR if the existing DPR does not match your requirements For data with images, users must be added to role with S3 access to the Zalando VOC skin tones dataset (first get DPR approval described above, then contact dataset owner) Retention \u00b6 Duration \u00b6 TBD Reasons for Duration \u00b6 TBD Policy Summary \u00b6 TBD Process Guide \u00b6 TBD Exception(s) and Exemption(s) \u00b6 TBD Deletion \u00b6 Deletion Event Summary \u00b6 TBD - One deletion event has occurred during the timespan of curating this dataset (is this documented elsewhere?) Acceptable Means of Deletion \u00b6 TBD Post-Deletion Obligations \u00b6 TBD Operational Requirement(s) \u00b6 TBD Exceptions and Exemptions \u00b6 TBD Provenance \u00b6 Collection \u00b6 Method(s) Used \u00b6 Taken from other existing datasets Crowdsourced - Internal Employee (See section on Annotations and Labeling ) Is this source considered sensitive or high-risk? Yes Dates of Collection: 2022/12/15 - 2023/03/15 Update Frequency for collected data: Static Additional Links for this collection: See section on Access, Rention, and Deletion Source Description(s) \u00b6 Source: Zalando VOC Skin Tones Dataset Collection Cadence \u00b6 Static: Data was collected once from a single source. Attribute Collection Criteria and Integration \u00b6 Data Integration \u00b6 Zalando-VOC images were used as input for labeling. These images are not generally included in skin tone dataset, but are identified by image-id to allow for fairness evaluation of systems that use such images. Data Point Collection Criteria \u00b6 Data Selection \u00b6 Filter out images with bad lighting and occlusions: done based with previously existing annotations done on Zalando VOC data Choose images with >2.5% skin exposure: This threshold gave balance between being able to see skin, and leaving enough images to annotate (~1000) for a fairness evaluation, given the annotation budget Relationship to Source \u00b6 Use and Utility(ies) \u00b6 Zalando VOC images: Skin tone labels data are intended to be used for to evaluate the fairness ML/AI systems that take Zalando VOC images as an input. Benefit and Value(s) \u00b6 Zalando VOC images: This data can be used to ensure ML/AI systems that consume Zalando VOC like images do not underperform for certain skin types. These skin tone data also inform others of existing skin tone biases in the Zalando VOC dataset. Limitation(s) and Trade-Off(s) \u00b6 Zalando VOC images: The skin tones in this dataset are annotations, not customer self-identifications . Skin tone annotation is subjective and the data here represent the best guesses from annotators that is affected a range of factors (see Annotations and Labeling ). Mistakes from labelers may have also occurred. Sensitive and Protected Attributes \u00b6 Sensitivity of Data \u00b6 Sensitivity Type(s) \u00b6 User Metadata (skin tones) Identifiable Data (unblurred images) S/PII Field(s) with Sensitive Data \u00b6 Intentional Collected Sensitive Data Images used in labeling contain pictures of customers (without blurred faces) Unintentionally Collected Sensitive Data Can see the setting in which customers take pictures of themselves Security and Privacy Handling \u00b6 Access to this data is restricted to a small select group of people as governed by the following Data Processing Requests (DPRs): DPR - Unblurred Zalando VOC Image access for Skin Tone Labeling : This DPR is associated with labeling jobs used to curate this dataset. Risk Type(s) \u00b6 See relevant DPRs Protected Attributes \u00b6 Protected Attribute Type(s) \u00b6 Skin Tone Field(s) with Protected Attributes \u00b6 Intentionally Collected Attributes Protected attributes were labeled or collected as a part of the dataset creation process. Field Name Description skin_tone_labels list of skin tone labels (one or more from each labeler) Unintentionally Collected Attributes Rationale \u00b6 To be used for fairness evaluation. Source(s) \u00b6 Methodology Detail(s) \u00b6 All protected attributes were collected via human annotation. See the Annotations and Labeling section for more details. Protected Attribute Distribution(s) \u00b6 All protected attributes (skin-tone) were collected via human annotation: See Annotations and Labeling - Distributions for skin-tone distributions. Known Correlations to Protected Attributes \u00b6 None identified at this time. Possible Correlations to Protected Attributes \u00b6 Labeler bias may cause correlations between skin tone labels and attributes of the image not related to skin tone, such as: judgements based on objects in the field of view (i.e. certain objects associated with certain cultures) facial shape and body shape repeated customers - some customers appear multiple times in separate images and labelers may have consistently given each the same incorrect skin tone labeler recency bias - seeing a lot of a certain skin color in a row can affect the next label made Risk(s) and Mitigation(s) \u00b6 Some of the possible correlations have been mitigated by: having multiple labelers from different backgrounds label each image shuffling the data each labeler labels - reducing the labeler recency bias effect making two independent labeler calibration round to have the chance to debug the labeling process have discussions about various unconscious labeler biases so each labeler can be mindful and potentially prevent introducing these unwanted correlations See Annotations and Labeling section for more details. Transformations \u00b6 Code Base and Existing Documentation \u00b6 See the skin-tone-labeling code base for code and documentation on data preparation, including data transformation. Synopsis \u00b6 Transformation(s) Applied \u00b6 Data Enrichment Grouping Library(ies) and Method(s) Used \u00b6 Transformation Type N/A Method: Skin tone labels have been grouped by image and enriched by calculating other statistics. No loss of raw data has taken place. Platforms, tools, or libraries: Python Additional Notes: Annotations and Labeling \u00b6 Annotation \u00b6 Task(s) \u00b6 Task description: Skin tone labelers were asked to label the skin tone of the human subject appearing in each image by following a particular set of Instructions for Labelers (more info about who the skin tone labelers where found below ). In short, each labeler labeled each image with one or more skin tone labels from the 2022 Zalando Beauty skin tone scale (shown below). LabelerAn uncertain label was also used by human-labelers that felt uncertain about the label they chose (for example, mid-light + uncertain , or just uncertain was allowed). Labelers were allowed to choose two adjacent labels when unsure (for example, mid-light + medium was allowed), and labelers had a separate label for indicating they were not sure of the correct label. The overall labeling work was broken down into four sequential tasks described in the following timeline figure: Figure: Labeling tasks. Labelers were given a two small calibration tasks and after each calibration task, a discussed took place about: How long did the task take? Were any instructions unclear? What, if any, potential biasing factors did you notice, and how might one be mindful about these to mitigate such biasing factors? Methods used: Four human labelers with varying backgrounds labeled each image (see section below on Human Annotators ). Inter-rater adjudication policy: Budget permitting, the next version of the dataset will include results from a labeler review of images where more than two labelers disagreed. Golden questions: No golden questions. Characteristic(s) \u00b6 Skin tone label Number Number of annotated examples 1058 Total number of annotations 4322 Average annotations per example 4.1 Number of annotators per example 4 4 of 4 agreement 12% 3 of 4 agreement 25% 2 of 4 agreement 51% 1 unique label 14% 2 unique labels 47% 3 unique labels 32% 4 unique labels 7% Above: Based on calibration #2 split (4 labelers labeling 59 examples). Label Consensus Statistics: Above: Based on calibration #2 split (4 labelers labeling 59 examples). See the Zalando-VOC Skin Tone dataset breakdown for more details. All statistics were calculated using this jupyter notebook . Description(s) \u00b6 Skin Tone Label Description: Skin tone annotations are subjective. Thus we worked with four labelers from different backgrounds who annotated for skin tone using the following Instructions for Labelers . Platforms, tools, or libraries: AWS SageMaker Distribution(s) \u00b6 Figure: Skin-tone label distributions (ignoring uncertain) for all task splits. Note, choice of neighbouring labels (i.e. mid-light + medium ) was allowed and such combinations are counted as distinct. All statistics were calculated using this jupyter notebook . Human Annotators \u00b6 Annotation Workforce Type \u00b6 Human Annotations (Expert) Human Annotations (Non-Expert) Human Annotations (Employees) Annotator Pool(s) \u00b6 Skin tone labelers (the only pool) Number of unique annotators: 4 Task(s) completed: This pool completed all tasks described above Expertise of annotators: Beauty Ethical data labeling Responsible AI Size and fit applications Summary of general (non task specific) annotation instructions: N/A Summary of annotator's responses to gold questions: N/A Annotation platforms: AWS SageMaker GroundTruth Language(s) \u00b6 (Annotator Languages Spoken) English [100 %] German [50 %] More TBD Location(s) \u00b6 (Annotator Locations of Upbringing) Canada [25 %] Azerbaijan [25 %] Germany [25 %] Iran [25 %] (Annotator Current Locations of Residence) Germany [75 %] Switzerland [25 %] Gender(s) \u00b6 (Annotator Genders) Male [50 %] Female [50 %] Validation Types \u00b6 Method(s) \u00b6 Range and Constraint Validation Structured Validation Consistency Validation Breakdown(s) \u00b6 (Validation Type) Number of Data Points Validated: all Description(s) \u00b6 All skin tone labels are checked for validity. Sampling Methods \u00b6 Method(s) Used \u00b6 Unsampled Characteristic(s) \u00b6 Sampling Criteria \u00b6 See Data Point Collection Criteria for information on how images were selected for labeling from the Zalando VOC Images Dataset . Documentation Metadata \u00b6 Note, some names in this document have been anonymized. Documentation Template Version \u00b6 v1.0.0 Documentation Authors \u00b6 Alex Loosley , Algorithmic Privacy and Fairness: (Principal Developer) Pak-Hang Wong , Algorithmic Privacy and Fairness: (Contributor)","title":"Skin Tones Data Documentation"},{"location":"example-data-documentation-voc-skin-tones/#skin-tones-data-documentation","text":"Dataset Owner : George Costanza Document Version : 0.5.0 Reviewers : Jerry Seinfeld Data documentation contributions and feedback are welcome!","title":"Skin Tones Data Documentation"},{"location":"example-data-documentation-voc-skin-tones/#overview","text":"","title":"Overview"},{"location":"example-data-documentation-voc-skin-tones/#primary-data-modality","text":"Image Data Tabular Data","title":"Primary Data Modality"},{"location":"example-data-documentation-voc-skin-tones/#dataset-description","text":"Post hoc skin tone labels of the faces of customers in the Zalando Voice of Customer (VOC) dataset curated as part of the \"Skin Tone Labeling Initiative.\" The primary purpose of this data is for fairness evaluation purposes: to help ensure data used to train ML/AI systems for Size and Fit is representative of Zalando's customers to ensure ML/AI systems do not systematically underperform for customers with certain skin tones (see Known Dataset Usage - Known Dataset Usage for more information about known fairness evaluations implemented using this dataset). This data was collected by a team of four Zalando labelers from a mix of teams including Beauty, Size and Fit, and Algorithmic Privacy and Fairness. Labelers followed specific Skin Tone Labeling Instructions , and labeled each image for skin tone based on the 2022 Zalando Beauty skin tone scale (shown below). Further technical details can be found in the Skin Tone Labeling GitHub Repository . Special thanks to the labelers: Alex Loosley , Algorithmic Privacy and Fairness Susan Ross , Algorithmic Privacy and Fairness Amrollah Seifoddini , Size and Fit Helen Seinfeld , Beauty Beyond fairness evaluations, this dataset along with the entire initiative has inpired the creation of a Zalando Skin Tone Labeling Playbook.","title":"Dataset Description"},{"location":"example-data-documentation-voc-skin-tones/#status","text":"Status Date: 27/04/2023 Under Preparation - The dataset is still under active curation and is not yet ready for use due to active \"dev\" updates. The dataset should be ready for wider usage in June 2023.","title":"Status"},{"location":"example-data-documentation-voc-skin-tones/#relevant-links","text":"Zalando VOC Skin Tones Dataset (S3 Bucket requiring access permissions) Zalando VOC Images Data Documentation (Base dataset used for labeling) Skin Tone Labeling Initiative Instructions for Labelers Dataset processing and analysis (GitHub Repository)","title":"Relevant Links"},{"location":"example-data-documentation-voc-skin-tones/#developers","text":"Alex Loosley , Algorithmic Privacy and Fairness: (Principal Developer) Amrollah Seifoddini , Size and Fit: (Dataset Owner)","title":"Developers"},{"location":"example-data-documentation-voc-skin-tones/#owner","text":"Main point of contact: George Steinbrenner Team: Size and Fit (Zurich) Affiliation: Zalando SE Team Website: N/A","title":"Owner"},{"location":"example-data-documentation-voc-skin-tones/#data-subjects","text":"Images of consenting customers Sensitive Data about people Skin tones labels","title":"Data Subject(s)"},{"location":"example-data-documentation-voc-skin-tones/#data-point-description","text":"A data point is made up of an image of a person, and one or more skin tone labels as defined in this Instructions for Labelers document. See the Examples of Data Points section for examples.","title":"Data Point Description"},{"location":"example-data-documentation-voc-skin-tones/#dataset-statistics","text":"Category Data Size of Dataset 1009 MB Number of Data Points 59 calibration + 999 main Label Classes 6 (5 skin tones, 1 for uncertainty) Type of labels Multiple labels per data point Algorithmic Labels 0 Human Labels All Additional Notes: The definitions of main and calibration splits and information on labels can be found in the Human Annotations section.","title":"Dataset Statistics"},{"location":"example-data-documentation-voc-skin-tones/#tables-and-fields","text":"","title":"Tables and Fields"},{"location":"example-data-documentation-voc-skin-tones/#table-labels_per_image","text":"Primary Key: annotation_image_id Description: Skin tone label data for each image. Field Name Type Description annotation_image_id str Primary key identifying image with respect to annotation job labels list[list[str]] List of multiple labels (inner lists) given by each labeler (outer list) skin_tone_values list[float] Skin tone values for each labeler (based on ZBeautySkinToneLabelEncoder ) valid_skin_tone_values list[float] Same as skin tone values with invalid value floats removed skin_tone_mean float Mean valid skin tone value skin_tone_std float Standard deviation of valid skin tone values Statistic skin_tone_mean count 999 mean 0.670921 std 0.629619 min 0. 25% 0.25 50% 0.5 75% 0.75 max 4. mode 0.5 Histogram of skin_tone_mean values: Additional Notes: Skin tone related labels and values are protected attributes, see the sensitive and protected attributes section for more details.","title":"TABLE: labels_per_image"},{"location":"example-data-documentation-voc-skin-tones/#table-labeling_job_manifest","text":"Primary Key: image_id Description: Annotation job manifest, containing information about what was annotated. Field Name Type Description image_id str Unique image id annotation_image_id str image id given corresponding annotation job source-ref str s3 bucket location","title":"TABLE: labeling_job_manifest"},{"location":"example-data-documentation-voc-skin-tones/#table-data_per_label","text":"Primary Key: None Description: Information about each label, such as a labeler UUID and meta data like how much time was needed to produce the label. Field Name Type Description annotation_image_id str links to image_id in labels_per_image table workerId str unique id of worker (labeler) who labeled the image timeSpentInSeconds float time needed for labeler to label the image","title":"TABLE: data_per_label"},{"location":"example-data-documentation-voc-skin-tones/#dataset-version-and-maintenance","text":"","title":"Dataset Version and Maintenance"},{"location":"example-data-documentation-voc-skin-tones/#version-details","text":"Current Data Version: Not currently tracked Data Version Release Date: 03/03/2023 Data Version for last Data Card Update: N/A Last Data Card Update: 10/03/2023","title":"Version Details"},{"location":"example-data-documentation-voc-skin-tones/#data-change-log","text":"TBD","title":"Data Change Log"},{"location":"example-data-documentation-voc-skin-tones/#maintenance-plan","text":"This dataset is in development mode and not yet being maintained for usage by others. Versioning: TBD Updates: TBD Errors: TBD Feedback: TBD","title":"Maintenance Plan"},{"location":"example-data-documentation-voc-skin-tones/#next-planned-updates","text":"Version affected: Not currently tracked Next data update: Ongoing until version 1.0.0 Next version: 1.0.0 Next version update: 04/2023","title":"Next Planned Update(s)"},{"location":"example-data-documentation-voc-skin-tones/#expected-changes","text":"Version 1.0.0 will be released once data curation and preparation is complete.","title":"Expected Change(s)"},{"location":"example-data-documentation-voc-skin-tones/#example-of-data-points","text":"","title":"Example of Data Points"},{"location":"example-data-documentation-voc-skin-tones/#typical-data-point","text":"A typical annotation example from the labels_per_image table: {\"annotation_image_id\": \"10\", \"skin_tone_values\": [3.5, 3.5, 3.0, 3.0], \"labels\": [[\"mid-deep\", \"deep\"], [\"mid-deep\", \"deep\"], [\"mid-deep\"], [\"mid-deep\"]], \"uncertain_labels\": [false, false, false, false], \"valid_skin_tone_values\": [3.5, 3.5, 3.0, 3.0], \"any_uncertainty\": false, \"complete_consensus\": false, \"overlap_consensus\": true, \"near_consensus\": true, \"n_skin_tone_values\": 4, \"frac_unique_values\": 0.5, \"n_unique_skin_tone_values\": 2, \"n_connected_skin_tone_groups\": 1, \"skin_tone_mean\": 3.25, \"skin_tone_std\": 0.25, \"has_invalid_annotation\": false, \"skin_tone_mode_value\": 3.0, \"skin_tone_mode_count\": 2, \"n_valid_labels\": 4} Here, annotators were not uncertain in their labels, two believed the skin tone was a combination of mid-deep and deep , and two thought the skin tone was just deep (note neighbouring skin-tone labels like mid-deep and deep were allowed, see Annotations and Labling for more details). Note, this data corresponds to the main labeling task which only had three labelers. See Annotations and Labeling for more details about labeling tasks.","title":"Typical Data Point"},{"location":"example-data-documentation-voc-skin-tones/#atypical-data-point","text":"An atypical annotation example from the labels_per_image table where the first labeler seemed to have made a mistake by labeled a skin tone as both light and deep : {\"annotation_image_id\": \"24\", \"skin_tone_values\": [-1.0, 1.5, 0.0], \"labels\": [[\"light\", \"deep\"], [\"mid-light\", \"medium\"], [\"light\"]], \"uncertain_labels\": [false, false, false], \"valid_skin_tone_values\": [1.5, 0.0], \"any_uncertainty\": false, \"complete_consensus\": false, \"overlap_consensus\": false, \"near_consensus\": false, \"n_skin_tone_values\": 2, \"frac_unique_values\": 1.0, \"n_unique_skin_tone_values\": 2, \"n_connected_skin_tone_groups\": 2, \"skin_tone_mean\": 0.75, \"skin_tone_std\": 0.75, \"has_invalid_annotation\": true, \"skin_tone_mode_value\": 0.0, \"skin_tone_mode_count\": 1} Note, this data corresponds to the calibration 1 labeling task which had more labeling mistakes as labelers got used to the labeling UI. See Annotations and Labeling for more details about labeling tasks.","title":"Atypical Data Point"},{"location":"example-data-documentation-voc-skin-tones/#sampling-of-data-points","text":"Example Type annotation_image_id Apparent skin tone human labelers agreed on labels 10 lighter 15 deeper human labelers were uncertain about label 14 lighter 255 deeper Additional Note : No actual images can be shown here because they require access approval (see Access section ).","title":"Sampling of Data Points"},{"location":"example-data-documentation-voc-skin-tones/#purpose-and-motivations","text":"","title":"Purpose and Motivations"},{"location":"example-data-documentation-voc-skin-tones/#intended-purposes","text":"Fairness Evaluation","title":"Intended Purpose(s)"},{"location":"example-data-documentation-voc-skin-tones/#motivating-factors","text":"Assessing and publishing the distribution of skin tones in the Zalando VOC dataset Identifying potential sample bias in data that may be used for training computer vision systems at Zalando Providing a skin tone dataset for fairness evaluation Writing a skin tone labeling playbook for others who want to curate skin tones via post hoc human labeling See the skin-tone-labeling repository for more details.","title":"Motivating Factor(s)"},{"location":"example-data-documentation-voc-skin-tones/#intended-use","text":"","title":"Intended Use"},{"location":"example-data-documentation-voc-skin-tones/#dataset-uses","text":"Skin tone fairness evalaution for pre-production models","title":"Dataset Use(s)"},{"location":"example-data-documentation-voc-skin-tones/#suitable-use-cases","text":"Suitable Use Case: Use to evaluate (un)fairness of any model that should perform well for Zalando VOC type images of humans. For example, Zalando's Body Measurements Pipeline .","title":"Suitable Use Case(s)"},{"location":"example-data-documentation-voc-skin-tones/#unsuitable-use-cases","text":"This data is, in its current form, not vetted for training a skin tone classifier that could be used at scale.","title":"Unsuitable Use Case(s)"},{"location":"example-data-documentation-voc-skin-tones/#research-and-problem-spaces","text":"Skin tone fairness evaluation Analysis of bias in human skin tone annotations","title":"Research and Problem Space(s)"},{"location":"example-data-documentation-voc-skin-tones/#information-for-usage","text":"","title":"Information for Usage"},{"location":"example-data-documentation-voc-skin-tones/#usage-guidelines","text":"Usage Guidelines: This dataset is meant for fairness evaluation purposes only to ensure that models trained on the Zalando VOC dataset, or similar, do not systematically underperform for subjects with certain skin tones. Approval Steps: The reason of using this dataset for a particular use case must be described and approved via a DPR process . New DPRs should refer to this existing DPR , which pertains to the creation of this dataset. See the Accesss Prerequesites section. Reviewer: Please tag the data owner when creating a DPR.","title":"Usage Guideline(s)"},{"location":"example-data-documentation-voc-skin-tones/#use-with-other-data","text":"","title":"Use with Other Data"},{"location":"example-data-documentation-voc-skin-tones/#safety-level","text":"Safe to use with other data for fairness evaluation purposes","title":"Safety Level"},{"location":"example-data-documentation-voc-skin-tones/#best-practices","text":"If presenting examples of this data is a must, consider blurring faces and backgrounds. Additional Notes: Add here","title":"Best Practices"},{"location":"example-data-documentation-voc-skin-tones/#forking-and-sampling","text":"","title":"Forking and Sampling"},{"location":"example-data-documentation-voc-skin-tones/#safety-level_1","text":"Conditionally safe to fork and/or sample","title":"Safety Level"},{"location":"example-data-documentation-voc-skin-tones/#acceptable-sampling-methods","text":"Cluster Sampling Multi-stage sampling Stratified Sampling Unsampled","title":"Acceptable Sampling Method(s)"},{"location":"example-data-documentation-voc-skin-tones/#best-practices_1","text":"This dataset is meant for fairness assessments against skin tone. Any samples should ensure that all skin tones are represented.","title":"Best Practice(s)"},{"location":"example-data-documentation-voc-skin-tones/#risks-and-mitigations","text":"Summarize here. Include links and metrics where applicable. Unrepresenting skin tone groups: Sampling incorrectly risks certain skin tone groups being underrepresented for skin tone based fairness evaluations. Ensure all skin tones are well represented such as to have enough data points to estimate performance on particular skin tones with a low enough level of uncertainty to be able to draw reliable fairness conclusions.","title":"Risk(s) and Mitigation(s)"},{"location":"example-data-documentation-voc-skin-tones/#notable-features","text":"Exploration Demo: Found in Jupyter Notebook","title":"Notable Feature(s)"},{"location":"example-data-documentation-voc-skin-tones/#distributions","text":"N/A - The entire main split of the dataset can be used for fairness evaluation. At this time, we do not recommend training models with this data, and therefore, do not have a recommended train-validation-test split.","title":"Distribution(s)"},{"location":"example-data-documentation-voc-skin-tones/#known-correlations","text":"None known at this time","title":"Known Correlation(s)"},{"location":"example-data-documentation-voc-skin-tones/#split-statistics","text":"TBD","title":"Split Statistics"},{"location":"example-data-documentation-voc-skin-tones/#citation-guidelines","text":"Guidelines: Refer to this dataset by it's title and provide a reference link to this data card.","title":"Citation Guidelines"},{"location":"example-data-documentation-voc-skin-tones/#known-usage","text":"","title":"Known Usage"},{"location":"example-data-documentation-voc-skin-tones/#modelss","text":"Model Model Task Purpose of Dataset Usage AI Act Risk Size and Fit - On Device Silhouette Extraction image segmentation Fairness Evaluation Limited Note, this table may not be exhaustive. Dataset users and documentation consumers at large are highly encouraged to contribute known usages.","title":"Models(s)"},{"location":"example-data-documentation-voc-skin-tones/#applications","text":"Application Brief Description Purpose of Dataset Usage AI Act Risk Size and Fit - Body Measurements Pipeline Pipeline from image of customer to body measurements including image segmentation and body reconstruction Fairnesse Evaluation Limited Size and Fit - Body Measurements Pipeline - 2022 Proof Of Concept An initial proof of concept to determine a best approach to doing fairness assessments on Size and Fit's body measurements pipeline Fairnesse Evaluation Limited Note, this table may not be exhaustive. Dataset users and documentation consumers at large are highly encouraged to contribute known usages.","title":"Application(s)"},{"location":"example-data-documentation-voc-skin-tones/#access-retention-and-deletion","text":"","title":"Access, Retention, and Deletion"},{"location":"example-data-documentation-voc-skin-tones/#access","text":"","title":"Access"},{"location":"example-data-documentation-voc-skin-tones/#relevant-links_1","text":"Zalando VOC Skin Tones Dataset (S3 Bucket requiring access permissions)","title":"Relevant Links"},{"location":"example-data-documentation-voc-skin-tones/#data-security-classification","text":"Yellow","title":"Data Security Classification"},{"location":"example-data-documentation-voc-skin-tones/#prerequisites","text":"Users requiring access must get approval on a DPR (with corresponding use case) either by: adding their user to the existing DPR creating a new DPR if the existing DPR does not match your requirements For data with images, users must be added to role with S3 access to the Zalando VOC skin tones dataset (first get DPR approval described above, then contact dataset owner)","title":"Prerequisite(s)"},{"location":"example-data-documentation-voc-skin-tones/#retention","text":"","title":"Retention"},{"location":"example-data-documentation-voc-skin-tones/#duration","text":"TBD","title":"Duration"},{"location":"example-data-documentation-voc-skin-tones/#reasons-for-duration","text":"TBD","title":"Reasons for Duration"},{"location":"example-data-documentation-voc-skin-tones/#policy-summary","text":"TBD","title":"Policy Summary"},{"location":"example-data-documentation-voc-skin-tones/#process-guide","text":"TBD","title":"Process Guide"},{"location":"example-data-documentation-voc-skin-tones/#exceptions-and-exemptions","text":"TBD","title":"Exception(s) and Exemption(s)"},{"location":"example-data-documentation-voc-skin-tones/#deletion","text":"","title":"Deletion"},{"location":"example-data-documentation-voc-skin-tones/#deletion-event-summary","text":"TBD - One deletion event has occurred during the timespan of curating this dataset (is this documented elsewhere?)","title":"Deletion Event Summary"},{"location":"example-data-documentation-voc-skin-tones/#acceptable-means-of-deletion","text":"TBD","title":"Acceptable Means of Deletion"},{"location":"example-data-documentation-voc-skin-tones/#post-deletion-obligations","text":"TBD","title":"Post-Deletion Obligations"},{"location":"example-data-documentation-voc-skin-tones/#operational-requirements","text":"TBD","title":"Operational Requirement(s)"},{"location":"example-data-documentation-voc-skin-tones/#exceptions-and-exemptions_1","text":"TBD","title":"Exceptions and Exemptions"},{"location":"example-data-documentation-voc-skin-tones/#provenance","text":"","title":"Provenance"},{"location":"example-data-documentation-voc-skin-tones/#collection","text":"","title":"Collection"},{"location":"example-data-documentation-voc-skin-tones/#methods-used","text":"Taken from other existing datasets Crowdsourced - Internal Employee (See section on Annotations and Labeling ) Is this source considered sensitive or high-risk? Yes Dates of Collection: 2022/12/15 - 2023/03/15 Update Frequency for collected data: Static Additional Links for this collection: See section on Access, Rention, and Deletion","title":"Method(s) Used"},{"location":"example-data-documentation-voc-skin-tones/#source-descriptions","text":"Source: Zalando VOC Skin Tones Dataset","title":"Source Description(s)"},{"location":"example-data-documentation-voc-skin-tones/#collection-cadence","text":"Static: Data was collected once from a single source.","title":"Collection Cadence"},{"location":"example-data-documentation-voc-skin-tones/#attribute-collection-criteria-and-integration","text":"","title":"Attribute Collection Criteria and Integration"},{"location":"example-data-documentation-voc-skin-tones/#data-integration","text":"Zalando-VOC images were used as input for labeling. These images are not generally included in skin tone dataset, but are identified by image-id to allow for fairness evaluation of systems that use such images.","title":"Data Integration"},{"location":"example-data-documentation-voc-skin-tones/#data-point-collection-criteria","text":"","title":"Data Point Collection Criteria"},{"location":"example-data-documentation-voc-skin-tones/#data-selection","text":"Filter out images with bad lighting and occlusions: done based with previously existing annotations done on Zalando VOC data Choose images with >2.5% skin exposure: This threshold gave balance between being able to see skin, and leaving enough images to annotate (~1000) for a fairness evaluation, given the annotation budget","title":"Data Selection"},{"location":"example-data-documentation-voc-skin-tones/#relationship-to-source","text":"","title":"Relationship to Source"},{"location":"example-data-documentation-voc-skin-tones/#use-and-utilityies","text":"Zalando VOC images: Skin tone labels data are intended to be used for to evaluate the fairness ML/AI systems that take Zalando VOC images as an input.","title":"Use and Utility(ies)"},{"location":"example-data-documentation-voc-skin-tones/#benefit-and-values","text":"Zalando VOC images: This data can be used to ensure ML/AI systems that consume Zalando VOC like images do not underperform for certain skin types. These skin tone data also inform others of existing skin tone biases in the Zalando VOC dataset.","title":"Benefit and Value(s)"},{"location":"example-data-documentation-voc-skin-tones/#limitations-and-trade-offs","text":"Zalando VOC images: The skin tones in this dataset are annotations, not customer self-identifications . Skin tone annotation is subjective and the data here represent the best guesses from annotators that is affected a range of factors (see Annotations and Labeling ). Mistakes from labelers may have also occurred.","title":"Limitation(s) and Trade-Off(s)"},{"location":"example-data-documentation-voc-skin-tones/#sensitive-and-protected-attributes","text":"","title":"Sensitive and Protected Attributes"},{"location":"example-data-documentation-voc-skin-tones/#sensitivity-of-data","text":"","title":"Sensitivity of Data"},{"location":"example-data-documentation-voc-skin-tones/#sensitivity-types","text":"User Metadata (skin tones) Identifiable Data (unblurred images) S/PII","title":"Sensitivity Type(s)"},{"location":"example-data-documentation-voc-skin-tones/#fields-with-sensitive-data","text":"Intentional Collected Sensitive Data Images used in labeling contain pictures of customers (without blurred faces) Unintentionally Collected Sensitive Data Can see the setting in which customers take pictures of themselves","title":"Field(s) with Sensitive Data"},{"location":"example-data-documentation-voc-skin-tones/#security-and-privacy-handling","text":"Access to this data is restricted to a small select group of people as governed by the following Data Processing Requests (DPRs): DPR - Unblurred Zalando VOC Image access for Skin Tone Labeling : This DPR is associated with labeling jobs used to curate this dataset.","title":"Security and Privacy Handling"},{"location":"example-data-documentation-voc-skin-tones/#risk-types","text":"See relevant DPRs","title":"Risk Type(s)"},{"location":"example-data-documentation-voc-skin-tones/#protected-attributes","text":"","title":"Protected Attributes"},{"location":"example-data-documentation-voc-skin-tones/#protected-attribute-types","text":"Skin Tone","title":"Protected Attribute Type(s)"},{"location":"example-data-documentation-voc-skin-tones/#fields-with-protected-attributes","text":"Intentionally Collected Attributes Protected attributes were labeled or collected as a part of the dataset creation process. Field Name Description skin_tone_labels list of skin tone labels (one or more from each labeler) Unintentionally Collected Attributes","title":"Field(s) with Protected Attributes"},{"location":"example-data-documentation-voc-skin-tones/#rationale","text":"To be used for fairness evaluation.","title":"Rationale"},{"location":"example-data-documentation-voc-skin-tones/#sources","text":"","title":"Source(s)"},{"location":"example-data-documentation-voc-skin-tones/#methodology-details","text":"All protected attributes were collected via human annotation. See the Annotations and Labeling section for more details.","title":"Methodology Detail(s)"},{"location":"example-data-documentation-voc-skin-tones/#protected-attribute-distributions","text":"All protected attributes (skin-tone) were collected via human annotation: See Annotations and Labeling - Distributions for skin-tone distributions.","title":"Protected Attribute Distribution(s)"},{"location":"example-data-documentation-voc-skin-tones/#known-correlations-to-protected-attributes","text":"None identified at this time.","title":"Known Correlations to Protected Attributes"},{"location":"example-data-documentation-voc-skin-tones/#possible-correlations-to-protected-attributes","text":"Labeler bias may cause correlations between skin tone labels and attributes of the image not related to skin tone, such as: judgements based on objects in the field of view (i.e. certain objects associated with certain cultures) facial shape and body shape repeated customers - some customers appear multiple times in separate images and labelers may have consistently given each the same incorrect skin tone labeler recency bias - seeing a lot of a certain skin color in a row can affect the next label made","title":"Possible Correlations to Protected Attributes"},{"location":"example-data-documentation-voc-skin-tones/#risks-and-mitigations_1","text":"Some of the possible correlations have been mitigated by: having multiple labelers from different backgrounds label each image shuffling the data each labeler labels - reducing the labeler recency bias effect making two independent labeler calibration round to have the chance to debug the labeling process have discussions about various unconscious labeler biases so each labeler can be mindful and potentially prevent introducing these unwanted correlations See Annotations and Labeling section for more details.","title":"Risk(s) and Mitigation(s)"},{"location":"example-data-documentation-voc-skin-tones/#transformations","text":"","title":"Transformations"},{"location":"example-data-documentation-voc-skin-tones/#code-base-and-existing-documentation","text":"See the skin-tone-labeling code base for code and documentation on data preparation, including data transformation.","title":"Code Base and Existing Documentation"},{"location":"example-data-documentation-voc-skin-tones/#synopsis","text":"","title":"Synopsis"},{"location":"example-data-documentation-voc-skin-tones/#transformations-applied","text":"Data Enrichment Grouping","title":"Transformation(s) Applied"},{"location":"example-data-documentation-voc-skin-tones/#libraryies-and-methods-used","text":"Transformation Type N/A Method: Skin tone labels have been grouped by image and enriched by calculating other statistics. No loss of raw data has taken place. Platforms, tools, or libraries: Python Additional Notes:","title":"Library(ies) and Method(s) Used"},{"location":"example-data-documentation-voc-skin-tones/#annotations-and-labeling","text":"","title":"Annotations and Labeling"},{"location":"example-data-documentation-voc-skin-tones/#annotation","text":"","title":"Annotation"},{"location":"example-data-documentation-voc-skin-tones/#tasks","text":"Task description: Skin tone labelers were asked to label the skin tone of the human subject appearing in each image by following a particular set of Instructions for Labelers (more info about who the skin tone labelers where found below ). In short, each labeler labeled each image with one or more skin tone labels from the 2022 Zalando Beauty skin tone scale (shown below). LabelerAn uncertain label was also used by human-labelers that felt uncertain about the label they chose (for example, mid-light + uncertain , or just uncertain was allowed). Labelers were allowed to choose two adjacent labels when unsure (for example, mid-light + medium was allowed), and labelers had a separate label for indicating they were not sure of the correct label. The overall labeling work was broken down into four sequential tasks described in the following timeline figure: Figure: Labeling tasks. Labelers were given a two small calibration tasks and after each calibration task, a discussed took place about: How long did the task take? Were any instructions unclear? What, if any, potential biasing factors did you notice, and how might one be mindful about these to mitigate such biasing factors? Methods used: Four human labelers with varying backgrounds labeled each image (see section below on Human Annotators ). Inter-rater adjudication policy: Budget permitting, the next version of the dataset will include results from a labeler review of images where more than two labelers disagreed. Golden questions: No golden questions.","title":"Task(s)"},{"location":"example-data-documentation-voc-skin-tones/#characteristics","text":"Skin tone label Number Number of annotated examples 1058 Total number of annotations 4322 Average annotations per example 4.1 Number of annotators per example 4 4 of 4 agreement 12% 3 of 4 agreement 25% 2 of 4 agreement 51% 1 unique label 14% 2 unique labels 47% 3 unique labels 32% 4 unique labels 7% Above: Based on calibration #2 split (4 labelers labeling 59 examples). Label Consensus Statistics: Above: Based on calibration #2 split (4 labelers labeling 59 examples). See the Zalando-VOC Skin Tone dataset breakdown for more details. All statistics were calculated using this jupyter notebook .","title":"Characteristic(s)"},{"location":"example-data-documentation-voc-skin-tones/#descriptions","text":"Skin Tone Label Description: Skin tone annotations are subjective. Thus we worked with four labelers from different backgrounds who annotated for skin tone using the following Instructions for Labelers . Platforms, tools, or libraries: AWS SageMaker","title":"Description(s)"},{"location":"example-data-documentation-voc-skin-tones/#distributions_1","text":"Figure: Skin-tone label distributions (ignoring uncertain) for all task splits. Note, choice of neighbouring labels (i.e. mid-light + medium ) was allowed and such combinations are counted as distinct. All statistics were calculated using this jupyter notebook .","title":"Distribution(s)"},{"location":"example-data-documentation-voc-skin-tones/#human-annotators","text":"","title":"Human Annotators"},{"location":"example-data-documentation-voc-skin-tones/#annotation-workforce-type","text":"Human Annotations (Expert) Human Annotations (Non-Expert) Human Annotations (Employees)","title":"Annotation Workforce Type"},{"location":"example-data-documentation-voc-skin-tones/#annotator-pools","text":"Skin tone labelers (the only pool) Number of unique annotators: 4 Task(s) completed: This pool completed all tasks described above Expertise of annotators: Beauty Ethical data labeling Responsible AI Size and fit applications Summary of general (non task specific) annotation instructions: N/A Summary of annotator's responses to gold questions: N/A Annotation platforms: AWS SageMaker GroundTruth","title":"Annotator Pool(s)"},{"location":"example-data-documentation-voc-skin-tones/#languages","text":"(Annotator Languages Spoken) English [100 %] German [50 %] More TBD","title":"Language(s)"},{"location":"example-data-documentation-voc-skin-tones/#locations","text":"(Annotator Locations of Upbringing) Canada [25 %] Azerbaijan [25 %] Germany [25 %] Iran [25 %] (Annotator Current Locations of Residence) Germany [75 %] Switzerland [25 %]","title":"Location(s)"},{"location":"example-data-documentation-voc-skin-tones/#genders","text":"(Annotator Genders) Male [50 %] Female [50 %]","title":"Gender(s)"},{"location":"example-data-documentation-voc-skin-tones/#validation-types","text":"","title":"Validation Types"},{"location":"example-data-documentation-voc-skin-tones/#methods","text":"Range and Constraint Validation Structured Validation Consistency Validation","title":"Method(s)"},{"location":"example-data-documentation-voc-skin-tones/#breakdowns","text":"(Validation Type) Number of Data Points Validated: all","title":"Breakdown(s)"},{"location":"example-data-documentation-voc-skin-tones/#descriptions_1","text":"All skin tone labels are checked for validity.","title":"Description(s)"},{"location":"example-data-documentation-voc-skin-tones/#sampling-methods","text":"","title":"Sampling Methods"},{"location":"example-data-documentation-voc-skin-tones/#methods-used_1","text":"Unsampled","title":"Method(s) Used"},{"location":"example-data-documentation-voc-skin-tones/#characteristics_1","text":"","title":"Characteristic(s)"},{"location":"example-data-documentation-voc-skin-tones/#sampling-criteria","text":"See Data Point Collection Criteria for information on how images were selected for labeling from the Zalando VOC Images Dataset .","title":"Sampling Criteria"},{"location":"example-data-documentation-voc-skin-tones/#documentation-metadata","text":"Note, some names in this document have been anonymized.","title":"Documentation Metadata"},{"location":"example-data-documentation-voc-skin-tones/#documentation-template-version","text":"v1.0.0","title":"Documentation Template Version"},{"location":"example-data-documentation-voc-skin-tones/#documentation-authors","text":"Alex Loosley , Algorithmic Privacy and Fairness: (Principal Developer) Pak-Hang Wong , Algorithmic Privacy and Fairness: (Contributor)","title":"Documentation Authors"},{"location":"example-model-documentation-alisnet/","text":"ALiSNet Model Documentation \u00b6 Model Owner : Amrollah Seifoddini Document Version : 0.2.0 Reviewers : Alex Loosley , Rocco Maresca Model documentation contributions and feedback are welcome! Overview \u00b6 Model Type \u00b6 Convolutional Neural Network Model Description \u00b6 A ccurate and Li ghtweight mobile human S egmentation Net work (ALiSNet) is a Convolutional Neural Network (CNN) for semantic segmentation of human silhouettes from photos. Architecture of ALiSNet is based on Semantic FPN with PointRend refinement. The original backbone of the network is exchanged by mobile-optimized MnasNet to achieve significant model size reduction. Additionaly, Feature Pyramid Network (FPN) from the original backbone is replaced with a simpler aggregation step, skipping FPN top-down path. Final model is also quantized to 8 bits integer precision using Qantization Aware Training. Status \u00b6 Status Date: 2025-01-22 Regularly Updated - New versions of the model have been or will continue to be made available. Relevant Links \u00b6 Paper Developer(s) \u00b6 Amrollah Seifoddini , Size & Fit Owner(s) \u00b6 Team Name : Size & Fit Contact Person : Amrollah Seifoddini Version Details and Artifacts \u00b6 Current Model Version: Not currently tracked Model Version Release Date: N/A Model Version for last Model Card Update: N/A Artifacts: Intended and Known Usage \u00b6 Intended Use \u00b6 Model was developed to perform semantic segmentation of humans vs. background as a step in inference of body measurements from photos. Given the specific use case, ALiSNet was trained to work for very narrow arm and leg angles ranges. Out of Scope Uses \u00b6 ALiSNet was trained to work for very narrow arm and leg angles ranges. It shouldn't be used for general semantic segmentation tasks of human silhouettes vs. background. Known Applications \u00b6 Application Purpose of Model Usage AI Act Risk Body Measurements Pipeline ALiSNet is used for silhouette extraction as part of Size & Fit's Body Measurements Pipeline. Limited Note, this table may not be exhaustive. Model users and documentation consumers at large are highly encouraged to contribute known usages. Risks and Ethical Considerations \u00b6 Privacy risks were initially reviewed by Size & Fit ( J. Peterman , Frank Constanza , David Puddy , Stefan Messmer ) in collaboration with Algorithmic Privacy & Fairness ( Elaine Benes ) and Product Security ( Kenny Bania ). Processing or storage of customer photos during deployment \u00b6 In order to obtain measurments customers need to upload their photos to our app so that their silhouette may be inferred. This creates a risk related to storage and processing of private data. Mitigation strategy: ALiSNet is deployed on-device. Customers photos are processed and stored on their device and Zalando does not have access to them. Only inferred silhouettes are sent to Zalando servers. On-device deployment \u00b6 Deploying ALiSNet on-device may cause risks related to backdoor attacks against ML models such as Membership Inference or Model Inversion which may result in violation of privacy of our customers. Mitigation strategy: Privacy risk assessment was performed in order to quantify Membership Inference (MIA) risk of the ALiSNet. As of November 2022 results of the audit showed that MIA risk is none to low. Biases in model performance \u00b6 Statistically relevant discrepancies in the model accuracy across protected attributes related to human characteristics such as age, percieved skin tone, height etc.. Mitigation Strategy: Fairnes assessment was performed in order to qunatify potential discerpancies across derived skin tone and body shape. As of October 2022 no relevant discrepencies in model performance were found. Training \u00b6 ALiSNet is a classification model trained on semantic segmentation task which goal is to perform a classification of pixels in the input image with corresponding semantic class (human, bike etc.). See ALiSNet Paper for more details about training. Datasets \u00b6 Name Location Sensitive* Size Filtered Common Objects in Context (COCO) s3://sagemaker-io-2/coco/ No* ~50k samples Another Production Dataset Training Split (Another Dataset Train) s3://obfuscated-bucket-name/production/ Yes* 3625 samples (*): Requires a special Data Processing Request Evaluation \u00b6 See ALiSNet Paper for more details about testing such as performance metrics and baselines. Performance Metrics \u00b6 Metrics used to evaluate model: MIoU : Mean Intersection over Union Results : Voices of Customers (VOC): MIoU : 97.6 (+/- 0.1) Model Bias and Fairness Analysis \u00b6 See Body Measurement Prediction Fairness Paper for complete fairness assessment results. Datasets \u00b6 ALiSNet was evaluated using multiple datasets. All of the datasets represent realistic examples expected to be seen in intended deployment scenario. Name Location Sensitive* Size Faces dataset 1 s3://obfuscated-bucket-name/faces/production/ Yes* 2416 samples Voices of Customers (VOC) dataset 1,2 s3://obfuscated-bucket-name/voc/production/ Yes* 6494 samples Optimass dataset 1 s3://obfuscated-bucket-name/optimass/production/ Yes* 322 samples Skin tones dataset 2 s3://obfuscated-bucket-name/skin-tones/production/ Yes* 499 samples (*): Requires a special Data Processing Request (1): Used for overall performance evaluation (2): Used for fairness evaluation Caveats and Recommendations \u00b6 Poor lightning conditions and low photo resolution may decrease quality of segmentation. (?) The model was designed with strong size and latency constraints and is ready to be deployed to production or edge devices. Documentation Metadata \u00b6 Note, some names in this document have been anonymized. Documentation Template Version \u00b6 v1.0.0 Documentation Authors \u00b6 Rocco Maresca , Algorithmic Privacy & Fairness: (Owner)","title":"ALiSNet Model Documentation"},{"location":"example-model-documentation-alisnet/#alisnet-model-documentation","text":"Model Owner : Amrollah Seifoddini Document Version : 0.2.0 Reviewers : Alex Loosley , Rocco Maresca Model documentation contributions and feedback are welcome!","title":"ALiSNet Model Documentation"},{"location":"example-model-documentation-alisnet/#overview","text":"","title":"Overview"},{"location":"example-model-documentation-alisnet/#model-type","text":"Convolutional Neural Network","title":"Model Type"},{"location":"example-model-documentation-alisnet/#model-description","text":"A ccurate and Li ghtweight mobile human S egmentation Net work (ALiSNet) is a Convolutional Neural Network (CNN) for semantic segmentation of human silhouettes from photos. Architecture of ALiSNet is based on Semantic FPN with PointRend refinement. The original backbone of the network is exchanged by mobile-optimized MnasNet to achieve significant model size reduction. Additionaly, Feature Pyramid Network (FPN) from the original backbone is replaced with a simpler aggregation step, skipping FPN top-down path. Final model is also quantized to 8 bits integer precision using Qantization Aware Training.","title":"Model Description"},{"location":"example-model-documentation-alisnet/#status","text":"Status Date: 2025-01-22 Regularly Updated - New versions of the model have been or will continue to be made available.","title":"Status"},{"location":"example-model-documentation-alisnet/#relevant-links","text":"Paper","title":"Relevant Links"},{"location":"example-model-documentation-alisnet/#developers","text":"Amrollah Seifoddini , Size & Fit","title":"Developer(s)"},{"location":"example-model-documentation-alisnet/#owners","text":"Team Name : Size & Fit Contact Person : Amrollah Seifoddini","title":"Owner(s)"},{"location":"example-model-documentation-alisnet/#version-details-and-artifacts","text":"Current Model Version: Not currently tracked Model Version Release Date: N/A Model Version for last Model Card Update: N/A Artifacts:","title":"Version Details and Artifacts"},{"location":"example-model-documentation-alisnet/#intended-and-known-usage","text":"","title":"Intended and Known Usage"},{"location":"example-model-documentation-alisnet/#intended-use","text":"Model was developed to perform semantic segmentation of humans vs. background as a step in inference of body measurements from photos. Given the specific use case, ALiSNet was trained to work for very narrow arm and leg angles ranges.","title":"Intended Use"},{"location":"example-model-documentation-alisnet/#out-of-scope-uses","text":"ALiSNet was trained to work for very narrow arm and leg angles ranges. It shouldn't be used for general semantic segmentation tasks of human silhouettes vs. background.","title":"Out of Scope Uses"},{"location":"example-model-documentation-alisnet/#known-applications","text":"Application Purpose of Model Usage AI Act Risk Body Measurements Pipeline ALiSNet is used for silhouette extraction as part of Size & Fit's Body Measurements Pipeline. Limited Note, this table may not be exhaustive. Model users and documentation consumers at large are highly encouraged to contribute known usages.","title":"Known Applications"},{"location":"example-model-documentation-alisnet/#risks-and-ethical-considerations","text":"Privacy risks were initially reviewed by Size & Fit ( J. Peterman , Frank Constanza , David Puddy , Stefan Messmer ) in collaboration with Algorithmic Privacy & Fairness ( Elaine Benes ) and Product Security ( Kenny Bania ).","title":"Risks and Ethical Considerations"},{"location":"example-model-documentation-alisnet/#processing-or-storage-of-customer-photos-during-deployment","text":"In order to obtain measurments customers need to upload their photos to our app so that their silhouette may be inferred. This creates a risk related to storage and processing of private data. Mitigation strategy: ALiSNet is deployed on-device. Customers photos are processed and stored on their device and Zalando does not have access to them. Only inferred silhouettes are sent to Zalando servers.","title":"Processing or storage of customer photos during deployment"},{"location":"example-model-documentation-alisnet/#on-device-deployment","text":"Deploying ALiSNet on-device may cause risks related to backdoor attacks against ML models such as Membership Inference or Model Inversion which may result in violation of privacy of our customers. Mitigation strategy: Privacy risk assessment was performed in order to quantify Membership Inference (MIA) risk of the ALiSNet. As of November 2022 results of the audit showed that MIA risk is none to low.","title":"On-device deployment"},{"location":"example-model-documentation-alisnet/#biases-in-model-performance","text":"Statistically relevant discrepancies in the model accuracy across protected attributes related to human characteristics such as age, percieved skin tone, height etc.. Mitigation Strategy: Fairnes assessment was performed in order to qunatify potential discerpancies across derived skin tone and body shape. As of October 2022 no relevant discrepencies in model performance were found.","title":"Biases in model performance"},{"location":"example-model-documentation-alisnet/#training","text":"ALiSNet is a classification model trained on semantic segmentation task which goal is to perform a classification of pixels in the input image with corresponding semantic class (human, bike etc.). See ALiSNet Paper for more details about training.","title":"Training"},{"location":"example-model-documentation-alisnet/#datasets","text":"Name Location Sensitive* Size Filtered Common Objects in Context (COCO) s3://sagemaker-io-2/coco/ No* ~50k samples Another Production Dataset Training Split (Another Dataset Train) s3://obfuscated-bucket-name/production/ Yes* 3625 samples (*): Requires a special Data Processing Request","title":"Datasets"},{"location":"example-model-documentation-alisnet/#evaluation","text":"See ALiSNet Paper for more details about testing such as performance metrics and baselines.","title":"Evaluation"},{"location":"example-model-documentation-alisnet/#performance-metrics","text":"Metrics used to evaluate model: MIoU : Mean Intersection over Union Results : Voices of Customers (VOC): MIoU : 97.6 (+/- 0.1)","title":"Performance Metrics"},{"location":"example-model-documentation-alisnet/#model-bias-and-fairness-analysis","text":"See Body Measurement Prediction Fairness Paper for complete fairness assessment results.","title":"Model Bias and Fairness Analysis"},{"location":"example-model-documentation-alisnet/#datasets_1","text":"ALiSNet was evaluated using multiple datasets. All of the datasets represent realistic examples expected to be seen in intended deployment scenario. Name Location Sensitive* Size Faces dataset 1 s3://obfuscated-bucket-name/faces/production/ Yes* 2416 samples Voices of Customers (VOC) dataset 1,2 s3://obfuscated-bucket-name/voc/production/ Yes* 6494 samples Optimass dataset 1 s3://obfuscated-bucket-name/optimass/production/ Yes* 322 samples Skin tones dataset 2 s3://obfuscated-bucket-name/skin-tones/production/ Yes* 499 samples (*): Requires a special Data Processing Request (1): Used for overall performance evaluation (2): Used for fairness evaluation","title":"Datasets"},{"location":"example-model-documentation-alisnet/#caveats-and-recommendations","text":"Poor lightning conditions and low photo resolution may decrease quality of segmentation. (?) The model was designed with strong size and latency constraints and is ready to be deployed to production or edge devices.","title":"Caveats and Recommendations"},{"location":"example-model-documentation-alisnet/#documentation-metadata","text":"Note, some names in this document have been anonymized.","title":"Documentation Metadata"},{"location":"example-model-documentation-alisnet/#documentation-template-version","text":"v1.0.0","title":"Documentation Template Version"},{"location":"example-model-documentation-alisnet/#documentation-authors","text":"Rocco Maresca , Algorithmic Privacy & Fairness: (Owner)","title":"Documentation Authors"},{"location":"template-application-documentation/","text":"Application Documentation Template \u00b6 Application Owner : Name and contact information Document Version : Version controlling this document is highly recommended Reviewers : List reviewers Key Links \u00b6 Code Repository Deployment Pipeline API ( Swagger Docs ) Cloud Account Project Management Board Application Architecture General Information \u00b6 EU AI Act Article 11 ; Annex IV paragraph 1, 2, 3 Purpose and Intended Use : Description of the AI system's intended purpose, including the sector of deployment. Clearly state the problem the AI application aims to solve. Delineate target users and stakeholders. Set measurable goals and key performance indicators (KPIs). Consider ethical implications and regulatory constraints. Clear statement on prohibited uses or potential misuse scenarios. Operational environment: Describe where and how the AI system will operate, such as on mobile devices, cloud platforms, or embedded systems. Risk classification \u00b6 Prohibited Risk: EU AI Act Chapter II Article 5 High-Risk: EU AI Act Chapter III, Section 1 Article 6 , Article 7 Limited Risk: Chapter IV Article 50 High / Limited / Minimal (in accordance with the AI Act) reasoning for the above classification Application Functionality \u00b6 EU AI Act Article 11 ; Annex IV , paragraph 1, 2, 3 Instructions for use for deployers : (EU AI Act Article 13 ) Model Capabilities : What the application can and cannot do (limitations). Supported languages, data types, or scenarios. Input Data Requirements : Format and quality expectations for input data. Examples of valid and invalid inputs. Output Explanation : How to interpret predictions, classifications, or recommendations. Uncertainty or confidence measures, if applicable. System Architecture Overview : Functional description and architecture of the system. Describe the key components of the system (including datasets, algorithms, models, etc.) Models and Datasets \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2 (d) Models \u00b6 Link to all model integrated in the AI/ML System Model Link to Single Source of Truth Description of Application Usage Model 1 TechOps Model Document ... Model 2 TechOps Model Document ... Model 3 GitHub Repo ... Datasets \u00b6 Link to all dataset documentation and information used to evaluate the AI/ML System. (Note, Model Documentation should also contain dataset information and links for all datasets used to train and test each respective model) Dataset Link to Single Source of Truth Description of Application Usage Dataset 1 TechOps Data Document ... Dataset 2 GitHub Repo ... Deployment \u00b6 Infrastructure and environment details (e.g., cloud setup, APIs). Integration with external systems or applications. Infrastructure and Environment Details \u00b6 Cloud Setup : Specify cloud provider (e.g., AWS, Azure, GCP) and regions. List required services: compute (e.g., EC2, Kubernetes), storage (e.g., S3, Blob Storage), and databases (e.g., DynamoDB, Firestore). Define resource configurations (e.g., VM sizes, GPU/TPU requirements). Network setup: VPC, subnets, and security groups. APIs : API endpoints, payload structure, authentication methods (e.g., OAuth, API keys). Latency and scalability expectations. Integration with External Systems \u00b6 EU AI Act Article 11 ; Annex IV paragraph 1 (b, c, d, g, h), 2 (a) Systems : List dependencies Data flow diagrams showing interactions. Error-handling mechanisms for APIs or webhooks Deployment Plan \u00b6 Infrastructure : List environments: development, staging, production. Resource scaling policies (e.g., autoscaling, redundancy). Backup and recovery processes. Integration Steps : Order of deployment (e.g., database migrations, model upload, service launch). Dependencies like libraries, frameworks, or APIs. Rollback strategies for each component. User Information : where is this under deployment? Lifecycle Management \u00b6 EU AI Act Article 11 ; Annex IV paragraph 6 Monitoring procedures for performance and ethical compliance. Versioning and change logs for model updates. Metrics : Application performance: response time, error rate. Model performance: accuracy, precision, recall. Infrastructure: CPU, memory, network usage. Key Activities : Monitor performance in real-world usage. Identify and fix drifts, bugs, or failures. Update the model periodically. Documentation Needs : Monitoring Logs : Real-time data on accuracy, latency, and uptime. Incident Reports : Record of failures, impacts, and resolutions. Retraining Logs : Data updates and changes in performance. Audit Trails : Comprehensive history of changes to ensure compliance. - Manteinance of change logs : new features added updates to existing functionality deprecated features removed features bug fixes security and vulnerability fixes Risk Management System \u00b6 EU AI Act Article 9 EU AI Act Article 11 ; Annex IV Risk Assessment Methodology: Describe the frameworks or standards used to identify and assess risks, such as ISO 31000 or failure mode and effects analysis (FMEA), or NIST Risk Assessment Framework. Identified Risks: Potential Harmful Outcomes: List possible negative effects, such as biased decisions, privacy breaches, or safety hazards. Likelihood and Severity: Assess how likely each risk is to occur and the potential impact on users or society. Risk Mitigation Measures \u00b6 Preventive Measures: Detail actions taken to prevent risks, like implementing data validation checks or bias reduction techniques. Protective Measures: Describe contingency plans and safeguards in place to minimize the impact if a risk materializes. Testing and Validation (Accuracy, Robustness, Cybersecurity) \u00b6 EU AI Act Article 15 Testing and Validation Procedures (Accuracy): Performance Metrics: List the metrics used to evaluate the AI system, such as accuracy, precision, recall, F1 score, or mean squared error. Validation Results: Summarize the outcomes of testing, including any benchmarks or thresholds met or exceeded. Measures for Accuracy: High-quality data, algorithm optimisation, evaluation metrics, and real-time performance tracking. Accuracy throughout the lifecycle \u00b6 Data Quality and Management: High-Quality Training Data: Data Preprocessing, techniques like normalisation, outlier removal, and feature scaling to improve data consistency, Data Augmentation, Data Validation Model Selection and Optimisation: Algorithm selection suited for the problem, Hyperparameter Tuning (grid search, random search, Bayesian optimization), Performance Validation( cross-validation by splitting data into training and testing sets, using k-fold or stratified cross-validation), Evaluation Metrics (precision,recall, F1 score, accuracy, mean squared error (MSE), or area under the curve (AUC). Feedback Mechanisms: Real-Time Error Tracking, Incorporate mechanisms to iteratively label and include challenging or misclassified examples for retraining. Robustness \u00b6 <-- Add outlier detection and all possible post analysis, what are the criticalities --> Robustness Measures: Adversarial training, stress testing, redundancy, error handling, and domain adaptation. Scenario-Based Testing: Plan for adversarial conditions, edge cases, and unusual input scenarios. Design the system to degrade gracefully when encountering unexpected inputs. Redundancy and Fail-Safes: Introduce fallback systems (e.g., rule-based or simpler models) to handle situations where the main AI system fails. Uncertainty Estimation: Include mechanisms to quantify uncertainty in the model\u2019s predictions (e.g., Bayesian networks or confidence scores). Cybersecurity \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2 (h) Data Security: Access Control: Incident Response : These measures include threat modelling, data security, adversarial robustness, secure development practices, access control, and incident response mechanisms. Post-deployment monitoring, patch management, and forensic logging are crucial to maintaining ongoing cybersecurity compliance. Documentation of all cybersecurity processes and incidents is mandatory to ensure accountability and regulatory conformity. Human Oversight \u00b6 EU AI Act Article 11 ;; Annex IV paragraph 2(e) EU AI Act Article 14 Human-in-the-Loop Mechanisms: Explain how human judgment is incorporated into the AI system\u2019s decision-making process, such as requiring human approval before action. Override and Intervention Procedures: Describe how users or operators can intervene or disable the AI system in case of errors or emergencies. User Instructions and Training: Provide guidelines and training materials to help users understand how to operate the AI system safely and effectively. Limitations and Constraints of the System: Clearly state what the AI system cannot do, including any known weaknesses or scenarios where performance may degrade. Incident Management \u00b6 Common Issues : List common errors and their solutions. Logs or debugging tips for advanced troubleshooting. Support Contact : How to reach technical support or community forums. Troubleshooting AI Application Deployment \u00b6 This section outlines potential issues that can arise during the deployment of an AI application, along with their causes, resolutions, and best practices for mitigation. Infrastructure-Level Issues \u00b6 Insufficient Resources \u00b6 Problem : Inaccurate resource estimation for production workloads. Unexpected spikes in user traffic can lead to insufficient resources such as compute, memory or storage that can lead to crashes and bad performance Mitigation Strategy : Network Failures \u00b6 Problem : network bottlenecks can lead to inaccessible or experiences latency of the application. Mitigation Strategy : Deployment Pipeline Failures \u00b6 Problem : pipeline fails to build, test, or deploy because of issues of compatibility between application code and infrastructure, environment variables or credentials misconfiguration. Mitigation Strategy : Integration Problems \u00b6 API Failures \u00b6 Problem : External APIs or internal services are unreachable due to network errors or authentication failures. Mitigation Strategy : Data Format Mismatches \u00b6 Problem : Crashes or errors due to unexpected data formats such as changes in the schema of external data sources or missing data validation steps. Mitigation Strategy : Data Quality Problems \u00b6 Problem : Inaccurate or corrupt data leads to poor predictions. Causes : No data validation or cleaning processes. Inconsistent labelling in training datasets. Mitigation Strategy : Model-Level Issues \u00b6 Performance or Deployment Issues \u00b6 Problem : Incorrect or inconsistent results due to data drift or inadequate training data for the real world deployment domain. Mitigation Strategy : Safety and Security Issues \u00b6 Unauthorised Access \u00b6 Problem : Sensitive data or APIs are exposed due to misconfigured authentication and authorization. Data Breaches \u00b6 Problem : User or model data is compromised due to insecure storage or lack of monitoring and logging of data access. Mitigation Strategy : Monitoring and Logging Failures \u00b6 Missing or Incomplete Logs \u00b6 Problem : Lack of information to debug issues due to inefficient logging. Critical issues go unnoticed, or too many false positives occur by lack of implementation ofactionable information in alerts. Mitigation Strategy : Recovery and Rollback \u00b6 Rollback Mechanisms \u00b6 Problem : New deployment introduces critical errors. Mitigation Strategy : Disaster Recovery \u00b6 Problem : Complete system outage or data loss. Mitigation Strategy : EU Declaration of conformity \u00b6 EU AI Act Article 47 Standards applied \u00b6 Documentation Metadata \u00b6 Template Version \u00b6 Documentation Authors \u00b6 Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager)","title":"Application Documentation Template"},{"location":"template-application-documentation/#application-documentation-template","text":"Application Owner : Name and contact information Document Version : Version controlling this document is highly recommended Reviewers : List reviewers","title":"Application Documentation Template"},{"location":"template-application-documentation/#key-links","text":"Code Repository Deployment Pipeline API ( Swagger Docs ) Cloud Account Project Management Board Application Architecture","title":"Key Links"},{"location":"template-application-documentation/#general-information","text":"EU AI Act Article 11 ; Annex IV paragraph 1, 2, 3 Purpose and Intended Use : Description of the AI system's intended purpose, including the sector of deployment. Clearly state the problem the AI application aims to solve. Delineate target users and stakeholders. Set measurable goals and key performance indicators (KPIs). Consider ethical implications and regulatory constraints. Clear statement on prohibited uses or potential misuse scenarios. Operational environment: Describe where and how the AI system will operate, such as on mobile devices, cloud platforms, or embedded systems.","title":"General Information"},{"location":"template-application-documentation/#risk-classification","text":"Prohibited Risk: EU AI Act Chapter II Article 5 High-Risk: EU AI Act Chapter III, Section 1 Article 6 , Article 7 Limited Risk: Chapter IV Article 50 High / Limited / Minimal (in accordance with the AI Act) reasoning for the above classification","title":"Risk classification"},{"location":"template-application-documentation/#application-functionality","text":"EU AI Act Article 11 ; Annex IV , paragraph 1, 2, 3 Instructions for use for deployers : (EU AI Act Article 13 ) Model Capabilities : What the application can and cannot do (limitations). Supported languages, data types, or scenarios. Input Data Requirements : Format and quality expectations for input data. Examples of valid and invalid inputs. Output Explanation : How to interpret predictions, classifications, or recommendations. Uncertainty or confidence measures, if applicable. System Architecture Overview : Functional description and architecture of the system. Describe the key components of the system (including datasets, algorithms, models, etc.)","title":"Application Functionality"},{"location":"template-application-documentation/#models-and-datasets","text":"EU AI Act Article 11 ; Annex IV paragraph 2 (d)","title":"Models and Datasets"},{"location":"template-application-documentation/#models","text":"Link to all model integrated in the AI/ML System Model Link to Single Source of Truth Description of Application Usage Model 1 TechOps Model Document ... Model 2 TechOps Model Document ... Model 3 GitHub Repo ...","title":"Models"},{"location":"template-application-documentation/#datasets","text":"Link to all dataset documentation and information used to evaluate the AI/ML System. (Note, Model Documentation should also contain dataset information and links for all datasets used to train and test each respective model) Dataset Link to Single Source of Truth Description of Application Usage Dataset 1 TechOps Data Document ... Dataset 2 GitHub Repo ...","title":"Datasets"},{"location":"template-application-documentation/#deployment","text":"Infrastructure and environment details (e.g., cloud setup, APIs). Integration with external systems or applications.","title":"Deployment"},{"location":"template-application-documentation/#infrastructure-and-environment-details","text":"Cloud Setup : Specify cloud provider (e.g., AWS, Azure, GCP) and regions. List required services: compute (e.g., EC2, Kubernetes), storage (e.g., S3, Blob Storage), and databases (e.g., DynamoDB, Firestore). Define resource configurations (e.g., VM sizes, GPU/TPU requirements). Network setup: VPC, subnets, and security groups. APIs : API endpoints, payload structure, authentication methods (e.g., OAuth, API keys). Latency and scalability expectations.","title":"Infrastructure and Environment Details"},{"location":"template-application-documentation/#integration-with-external-systems","text":"EU AI Act Article 11 ; Annex IV paragraph 1 (b, c, d, g, h), 2 (a) Systems : List dependencies Data flow diagrams showing interactions. Error-handling mechanisms for APIs or webhooks","title":"Integration with External Systems"},{"location":"template-application-documentation/#deployment-plan","text":"Infrastructure : List environments: development, staging, production. Resource scaling policies (e.g., autoscaling, redundancy). Backup and recovery processes. Integration Steps : Order of deployment (e.g., database migrations, model upload, service launch). Dependencies like libraries, frameworks, or APIs. Rollback strategies for each component. User Information : where is this under deployment?","title":"Deployment Plan"},{"location":"template-application-documentation/#lifecycle-management","text":"EU AI Act Article 11 ; Annex IV paragraph 6 Monitoring procedures for performance and ethical compliance. Versioning and change logs for model updates. Metrics : Application performance: response time, error rate. Model performance: accuracy, precision, recall. Infrastructure: CPU, memory, network usage. Key Activities : Monitor performance in real-world usage. Identify and fix drifts, bugs, or failures. Update the model periodically. Documentation Needs : Monitoring Logs : Real-time data on accuracy, latency, and uptime. Incident Reports : Record of failures, impacts, and resolutions. Retraining Logs : Data updates and changes in performance. Audit Trails : Comprehensive history of changes to ensure compliance. - Manteinance of change logs : new features added updates to existing functionality deprecated features removed features bug fixes security and vulnerability fixes","title":"Lifecycle Management"},{"location":"template-application-documentation/#risk-management-system","text":"EU AI Act Article 9 EU AI Act Article 11 ; Annex IV Risk Assessment Methodology: Describe the frameworks or standards used to identify and assess risks, such as ISO 31000 or failure mode and effects analysis (FMEA), or NIST Risk Assessment Framework. Identified Risks: Potential Harmful Outcomes: List possible negative effects, such as biased decisions, privacy breaches, or safety hazards. Likelihood and Severity: Assess how likely each risk is to occur and the potential impact on users or society.","title":"Risk Management System"},{"location":"template-application-documentation/#risk-mitigation-measures","text":"Preventive Measures: Detail actions taken to prevent risks, like implementing data validation checks or bias reduction techniques. Protective Measures: Describe contingency plans and safeguards in place to minimize the impact if a risk materializes.","title":"Risk Mitigation Measures"},{"location":"template-application-documentation/#testing-and-validation-accuracy-robustness-cybersecurity","text":"EU AI Act Article 15 Testing and Validation Procedures (Accuracy): Performance Metrics: List the metrics used to evaluate the AI system, such as accuracy, precision, recall, F1 score, or mean squared error. Validation Results: Summarize the outcomes of testing, including any benchmarks or thresholds met or exceeded. Measures for Accuracy: High-quality data, algorithm optimisation, evaluation metrics, and real-time performance tracking.","title":"Testing and Validation (Accuracy, Robustness, Cybersecurity)"},{"location":"template-application-documentation/#accuracy-throughout-the-lifecycle","text":"Data Quality and Management: High-Quality Training Data: Data Preprocessing, techniques like normalisation, outlier removal, and feature scaling to improve data consistency, Data Augmentation, Data Validation Model Selection and Optimisation: Algorithm selection suited for the problem, Hyperparameter Tuning (grid search, random search, Bayesian optimization), Performance Validation( cross-validation by splitting data into training and testing sets, using k-fold or stratified cross-validation), Evaluation Metrics (precision,recall, F1 score, accuracy, mean squared error (MSE), or area under the curve (AUC). Feedback Mechanisms: Real-Time Error Tracking, Incorporate mechanisms to iteratively label and include challenging or misclassified examples for retraining.","title":"Accuracy throughout the lifecycle"},{"location":"template-application-documentation/#robustness","text":"<-- Add outlier detection and all possible post analysis, what are the criticalities --> Robustness Measures: Adversarial training, stress testing, redundancy, error handling, and domain adaptation. Scenario-Based Testing: Plan for adversarial conditions, edge cases, and unusual input scenarios. Design the system to degrade gracefully when encountering unexpected inputs. Redundancy and Fail-Safes: Introduce fallback systems (e.g., rule-based or simpler models) to handle situations where the main AI system fails. Uncertainty Estimation: Include mechanisms to quantify uncertainty in the model\u2019s predictions (e.g., Bayesian networks or confidence scores).","title":"Robustness"},{"location":"template-application-documentation/#cybersecurity","text":"EU AI Act Article 11 ; Annex IV paragraph 2 (h) Data Security: Access Control: Incident Response : These measures include threat modelling, data security, adversarial robustness, secure development practices, access control, and incident response mechanisms. Post-deployment monitoring, patch management, and forensic logging are crucial to maintaining ongoing cybersecurity compliance. Documentation of all cybersecurity processes and incidents is mandatory to ensure accountability and regulatory conformity.","title":"Cybersecurity"},{"location":"template-application-documentation/#human-oversight","text":"EU AI Act Article 11 ;; Annex IV paragraph 2(e) EU AI Act Article 14 Human-in-the-Loop Mechanisms: Explain how human judgment is incorporated into the AI system\u2019s decision-making process, such as requiring human approval before action. Override and Intervention Procedures: Describe how users or operators can intervene or disable the AI system in case of errors or emergencies. User Instructions and Training: Provide guidelines and training materials to help users understand how to operate the AI system safely and effectively. Limitations and Constraints of the System: Clearly state what the AI system cannot do, including any known weaknesses or scenarios where performance may degrade.","title":"Human Oversight"},{"location":"template-application-documentation/#incident-management","text":"Common Issues : List common errors and their solutions. Logs or debugging tips for advanced troubleshooting. Support Contact : How to reach technical support or community forums.","title":"Incident Management"},{"location":"template-application-documentation/#troubleshooting-ai-application-deployment","text":"This section outlines potential issues that can arise during the deployment of an AI application, along with their causes, resolutions, and best practices for mitigation.","title":"Troubleshooting AI Application Deployment"},{"location":"template-application-documentation/#infrastructure-level-issues","text":"","title":"Infrastructure-Level Issues"},{"location":"template-application-documentation/#insufficient-resources","text":"Problem : Inaccurate resource estimation for production workloads. Unexpected spikes in user traffic can lead to insufficient resources such as compute, memory or storage that can lead to crashes and bad performance Mitigation Strategy :","title":"Insufficient Resources"},{"location":"template-application-documentation/#network-failures","text":"Problem : network bottlenecks can lead to inaccessible or experiences latency of the application. Mitigation Strategy :","title":"Network Failures"},{"location":"template-application-documentation/#deployment-pipeline-failures","text":"Problem : pipeline fails to build, test, or deploy because of issues of compatibility between application code and infrastructure, environment variables or credentials misconfiguration. Mitigation Strategy :","title":"Deployment Pipeline Failures"},{"location":"template-application-documentation/#integration-problems","text":"","title":"Integration Problems"},{"location":"template-application-documentation/#api-failures","text":"Problem : External APIs or internal services are unreachable due to network errors or authentication failures. Mitigation Strategy :","title":"API Failures"},{"location":"template-application-documentation/#data-format-mismatches","text":"Problem : Crashes or errors due to unexpected data formats such as changes in the schema of external data sources or missing data validation steps. Mitigation Strategy :","title":"Data Format Mismatches"},{"location":"template-application-documentation/#data-quality-problems","text":"Problem : Inaccurate or corrupt data leads to poor predictions. Causes : No data validation or cleaning processes. Inconsistent labelling in training datasets. Mitigation Strategy :","title":"Data Quality Problems"},{"location":"template-application-documentation/#model-level-issues","text":"","title":"Model-Level Issues"},{"location":"template-application-documentation/#performance-or-deployment-issues","text":"Problem : Incorrect or inconsistent results due to data drift or inadequate training data for the real world deployment domain. Mitigation Strategy :","title":"Performance or Deployment Issues"},{"location":"template-application-documentation/#safety-and-security-issues","text":"","title":"Safety and Security Issues"},{"location":"template-application-documentation/#unauthorised-access","text":"Problem : Sensitive data or APIs are exposed due to misconfigured authentication and authorization.","title":"Unauthorised Access"},{"location":"template-application-documentation/#data-breaches","text":"Problem : User or model data is compromised due to insecure storage or lack of monitoring and logging of data access. Mitigation Strategy :","title":"Data Breaches"},{"location":"template-application-documentation/#monitoring-and-logging-failures","text":"","title":"Monitoring and Logging Failures"},{"location":"template-application-documentation/#missing-or-incomplete-logs","text":"Problem : Lack of information to debug issues due to inefficient logging. Critical issues go unnoticed, or too many false positives occur by lack of implementation ofactionable information in alerts. Mitigation Strategy :","title":"Missing or Incomplete Logs"},{"location":"template-application-documentation/#recovery-and-rollback","text":"","title":"Recovery and Rollback"},{"location":"template-application-documentation/#rollback-mechanisms","text":"Problem : New deployment introduces critical errors. Mitigation Strategy :","title":"Rollback Mechanisms"},{"location":"template-application-documentation/#disaster-recovery","text":"Problem : Complete system outage or data loss. Mitigation Strategy :","title":"Disaster Recovery"},{"location":"template-application-documentation/#eu-declaration-of-conformity","text":"EU AI Act Article 47","title":"EU Declaration of conformity"},{"location":"template-application-documentation/#standards-applied","text":"","title":"Standards applied"},{"location":"template-application-documentation/#documentation-metadata","text":"","title":"Documentation Metadata"},{"location":"template-application-documentation/#template-version","text":"","title":"Template Version"},{"location":"template-application-documentation/#documentation-authors","text":"Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager)","title":"Documentation Authors"},{"location":"template-data-documentation/","text":"Data Documentation Template \u00b6 EU AI Act Article 10 EU AI Act Article 11 ; Annex IV paragraph 1, 2 (d) Dataset Owner : Name and contact information Document Version : Version controlling this document is highly recommended Reviewers : List reviewers Overview \u00b6 Dataset Description \u00b6 EU AI Act Article 11 ; Annex IV paragraph 1, 2(d) Write a short summary describing your dataset (limit 200 words). Include information about the content and topic of the data, sources and motivations for the dataset, benefits and the problems or use cases it is suitable for. For readers that only take 10 seconds to look at this data card, adding one good overview image might also make the difference between this data being discovered and going unnoticed. For more tips on describing data, see Zalando Data Foundation's Quality of Data Descriptions ! Status \u00b6 Status Date: YYYY-MM-DD Status: specify one of : Under Preparation -- The dataset is still under active curation and is not yet ready for use due to active \"dev\" updates. Regularly Updated -- New versions of the dataset have been or will continue to be made available. Actively Maintained -- No new versions will be made available, but this dataset will be actively maintained, including but not limited to updates to the data. Limited Maintenance -- The data will not be updated, but any technical issues will be addressed. Deprecated -- This dataset is obsolete or is no longer being maintained. Relevant Links \u00b6 Example references: GitHub Repository Paper/Documentation Link Initiative Demo Conference Talk API Link Developers \u00b6 Name, Team Name, Team Owner \u00b6 Team Name, Contact Person Deployer instructions of Use \u00b6 Instructions for use for deployers : EU AI Act Article 13 Version Details \u00b6 Data Versioning \u00b6 (Article 11, paragraph 2(d)) Data Version Control Tools: Include a Data_versioning.md file to document changes DVC (Data Version Control): Tracks datasets, connects them to model versions, and integrates with Git. Git-LFS (Large File Storage): Stores large data files outside the Git repository. Maintenance of Metadata and Schema Versioning \u00b6 EU AI Act Article 11 ; Annex IV paragraph 3 Why \u00b6 Data formats, schema, and other metadata changes can impact downstream processes. Tracking these ensures transparency. How \u00b6 Create a data dictionary: Document dataset structure, column descriptions, data types, and relationships. Track schema changes: Use tools to log schema evolution. Record changes as part of version control or data pipelines. Save metadata alongside datasets: Include details like source, timestamp, description, version, and quality metrics. Known Usages \u00b6 EU AI Act Article 11 ; Annex IV paragraph 3 Model(s) \u00b6 Model Model Task Purpose of Dataset Usage Example Model 1 Image Segmentation Fairness evaluation Example Model 2 Skin Tone Classifier Training and validation Note, this table does not have to be exhaustive. Dataset users and documentation consumers at large are highly encouraged to contribute known usages. Application(s) \u00b6 Application Brief Description Purpose of Dataset Usage Example Application 1 Size and Fit Recommendations Fairness Evaluation of end-to-end application pipeline Dataset Characteristics \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2(d) Data Types: (e.g., images, text, audio, structured, unstructured data, personal data) Size/Volume: Number of Instances/Records: Primary Use Case(s): Description of the main AI use cases that the dataset was designed for or is typically used in. Associated AI System(s): List known AI system(s) that this dataset is or has been used in. Number of Features/Attributes (if applicable): Label Information (if applicable): Geographical Scope: Geographic location(s) where the data was collected. Date of Collection: Start and end date of data collection. Data Origin and Source \u00b6 Source(s): Provide information about where the data was sourced from (e.g.,public datasets, sensors, surveys, web scraping, crowdsourced). Third-Party Data: Indicate if any part of the dataset was obtained from third parties, and if so, detail the legal agreements in place (license, usage rights, etc.). Ethical Sourcing: Provide information on the ethical and legal compliance of the data collection process (e.g., informed consent, transparency to data subjects, and compliance with GDPR or other regulations). Provenance \u00b6 Describe the history and origin of the data. Collection \u00b6 Method(s) Used \u00b6 Specify one or more of: API Artificially generated Crowdsourced - Internal Employee Crowdsourced - External Paid Crowdsourced - Volunteer Vendor collection efforts Scraped or crawled Survey, forms, or polls Interviews, focus groups Scientific experiment Taken from other existing datasets Unknown To be determined Others (please specify) Methodology Detail(s) \u00b6 EU AI Act Article 11 ; Annex IV 2 (a), (b), (d) Collection Type Source: Describe here. Include links where available. Platform: [Platform Name], Describe platform here. Include links where relevant. Is this source considered sensitive or high-risk? [Yes/No] Dates of Collection: [YYYY-MM -- YYYY-MM] Update Frequency for collected data: Select one for this collection type: yearly, quarterly, monthly, on demand, no changes, others, .... Additional Links for this collection: See section on Access, Rention, and Deletion Additional Notes: Add here Source Description(s) \u00b6 Source: Describe here. Include links, data examples, metrics, visualizations where relevant. Source: Describe here. Include links, data examples, metrics, visualizations where relevant. Source: Describe here. Include links, data examples, metrics, visualizations where relevant. Additional Notes: Add here Collection Cadence \u00b6 Static: Data was collected once from single or multiple sources. Streamed: Data is continuously acquired from single or multiple sources. Dynamic: Data is updated regularly from single or multiple sources. Others: Please specify Data Pre-Processing \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2 (d, e) Data Cleaning \u00b6 Handling missing data: (e.g., removal, imputation method used) Outlier treatment: (e.g., detection and removal technique) Duplicates removal: (Yes/No) Error correction: (Manual/Automated, if applicable) Data Transformation \u00b6 Normalization/Standardization: (Method used, e.g., min-max scaling) Encoding categorical data: (e.g., one-hot encoding, label encoding) Text/tokenization: (Applicable for NLP tasks) Feature Engineering \u00b6 Feature selection: (e.g., methods used to select features) Feature extraction: (e.g., PCA, interaction terms) Newly created features: (List any) Dimensionality Reduction \u00b6 Technique(s) used: (e.g., PCA, t-SNE) Number of dimensions after reduction: (Specify) Data Augmentation \u00b6 Augmentation technique(s): (e.g., rotation, flipping for images) Data Annotation and Labeling \u00b6 EU AI Act Article 11 ; Annex IV 2(d) Annotation Process: Describe the process used to label or annotate the data (e.g., human labelers, automated, crowdsourcing). Annotation platform Validation: Explain any quality control mechanisms applied to ensure accurate labeling or annotation Inter-Annotator agreement Consensus process Calibration rounds Annotator Demographics (Location / Language / Expertise / Background) Validation Types \u00b6 Method(s) \u00b6 Example= range and constraint validation, structured validation, consistency validation Breakdown(s) \u00b6 (Validation Type) Number of Data Points Validated: Description(s) \u00b6 Sampling Methods \u00b6 Method(s) Used \u00b6 Characteristic(s) \u00b6 Sampling Criteria \u00b6 Description(s) \u00b6 Dataset Distribution and Licensing \u00b6 EU AI Act Article 11 ; Annex IV 2(d) Availability: Open/public or private dataset Dataset Documentation Link: (Link to further details if available) User Rights and Limitations: Access, Retention, and Deletion \u00b6 Access \u00b6 Relevant Links \u00b6 [Link to filestore] [Link to governance processes for data access] ... Data Security Classification in and out of scope delineation \u00b6 Prerequisite(s) \u00b6 For example: This dataset requires membership in [specific] database groups: Complete the [Mandatory Training] Read [Data Usage Policy] Initiate a [Data Processing Request] Retention \u00b6 Duration \u00b6 Specify duration in days, months, or years. Reasons for Duration \u00b6 ... Policy Summary \u00b6 Policy: Add a link to the policy if it's standardized at your company Data Risk Assessment \u00b6 Describe the assessment of data risks : Foreseeable unintended outcomes or biases arising from dataset use. Sources of potential discrimination or harm. Cybersecurity Measures \u00b6 EU AI Act Article 11 ; Annex IV paragraph 5 Data Security Measures \u00b6 Data Storage \u00b6 Encryption : Use AES-256; detail key management (e.g., HSM, key rotation). Access Control : Implement role-based access and MFA. Backup : Document backup frequency, encryption, and recovery testing. Integrity Monitoring : Use hashes, checksums, or blockchain. Security : Describe server protections (e.g., restricted access). Data Transfer \u00b6 Encryption in Transit : Specify TLS 1.3, IPsec configurations. Endpoint Security : Detail device verification and certificate pinning. API Security : Document authentication, rate-limiting, and channel encryption. Data Masking : Use pseudonymisation for sensitive data in transit. Data Processing \u00b6 Secure Environments : Use containers, VMs, or trusted execution (e.g., Intel SGX). Audit Logs : Specify logging standards, retention, and tamper protection. Data Minimisation : Anonymise or limit collected data. Standards Applied \u00b6 Data post-market monitoring \u00b6 - Data Drift Detection and Monitoring: Describe here what type of drift was identified (covariate drift, prior probability drift or concept drift) - Audit Logs: Periodically perform manual or semi-automated reviews of data samples and log changes in the data as well as access patterns. Action plans implemented to address identified issues: . EU Declaration of conformity \u00b6 EU AI Act Article 47 Standards applied \u00b6 Documentation Metadata \u00b6 Version \u00b6 Template Version \u00b6 Documentation Authors \u00b6 Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager)","title":"Data Documentation Template"},{"location":"template-data-documentation/#data-documentation-template","text":"EU AI Act Article 10 EU AI Act Article 11 ; Annex IV paragraph 1, 2 (d) Dataset Owner : Name and contact information Document Version : Version controlling this document is highly recommended Reviewers : List reviewers","title":"Data Documentation Template"},{"location":"template-data-documentation/#overview","text":"","title":"Overview"},{"location":"template-data-documentation/#dataset-description","text":"EU AI Act Article 11 ; Annex IV paragraph 1, 2(d) Write a short summary describing your dataset (limit 200 words). Include information about the content and topic of the data, sources and motivations for the dataset, benefits and the problems or use cases it is suitable for. For readers that only take 10 seconds to look at this data card, adding one good overview image might also make the difference between this data being discovered and going unnoticed. For more tips on describing data, see Zalando Data Foundation's Quality of Data Descriptions !","title":"Dataset Description"},{"location":"template-data-documentation/#status","text":"Status Date: YYYY-MM-DD Status: specify one of : Under Preparation -- The dataset is still under active curation and is not yet ready for use due to active \"dev\" updates. Regularly Updated -- New versions of the dataset have been or will continue to be made available. Actively Maintained -- No new versions will be made available, but this dataset will be actively maintained, including but not limited to updates to the data. Limited Maintenance -- The data will not be updated, but any technical issues will be addressed. Deprecated -- This dataset is obsolete or is no longer being maintained.","title":"Status"},{"location":"template-data-documentation/#relevant-links","text":"Example references: GitHub Repository Paper/Documentation Link Initiative Demo Conference Talk API Link","title":"Relevant Links"},{"location":"template-data-documentation/#developers","text":"Name, Team Name, Team","title":"Developers"},{"location":"template-data-documentation/#owner","text":"Team Name, Contact Person","title":"Owner"},{"location":"template-data-documentation/#deployer-instructions-of-use","text":"Instructions for use for deployers : EU AI Act Article 13","title":"Deployer instructions of Use"},{"location":"template-data-documentation/#version-details","text":"","title":"Version Details"},{"location":"template-data-documentation/#data-versioning","text":"(Article 11, paragraph 2(d)) Data Version Control Tools: Include a Data_versioning.md file to document changes DVC (Data Version Control): Tracks datasets, connects them to model versions, and integrates with Git. Git-LFS (Large File Storage): Stores large data files outside the Git repository.","title":"Data Versioning"},{"location":"template-data-documentation/#maintenance-of-metadata-and-schema-versioning","text":"EU AI Act Article 11 ; Annex IV paragraph 3","title":"Maintenance of Metadata and Schema Versioning"},{"location":"template-data-documentation/#why","text":"Data formats, schema, and other metadata changes can impact downstream processes. Tracking these ensures transparency.","title":"Why"},{"location":"template-data-documentation/#how","text":"Create a data dictionary: Document dataset structure, column descriptions, data types, and relationships. Track schema changes: Use tools to log schema evolution. Record changes as part of version control or data pipelines. Save metadata alongside datasets: Include details like source, timestamp, description, version, and quality metrics.","title":"How"},{"location":"template-data-documentation/#known-usages","text":"EU AI Act Article 11 ; Annex IV paragraph 3","title":"Known Usages"},{"location":"template-data-documentation/#models","text":"Model Model Task Purpose of Dataset Usage Example Model 1 Image Segmentation Fairness evaluation Example Model 2 Skin Tone Classifier Training and validation Note, this table does not have to be exhaustive. Dataset users and documentation consumers at large are highly encouraged to contribute known usages.","title":"Model(s)"},{"location":"template-data-documentation/#applications","text":"Application Brief Description Purpose of Dataset Usage Example Application 1 Size and Fit Recommendations Fairness Evaluation of end-to-end application pipeline","title":"Application(s)"},{"location":"template-data-documentation/#dataset-characteristics","text":"EU AI Act Article 11 ; Annex IV paragraph 2(d) Data Types: (e.g., images, text, audio, structured, unstructured data, personal data) Size/Volume: Number of Instances/Records: Primary Use Case(s): Description of the main AI use cases that the dataset was designed for or is typically used in. Associated AI System(s): List known AI system(s) that this dataset is or has been used in. Number of Features/Attributes (if applicable): Label Information (if applicable): Geographical Scope: Geographic location(s) where the data was collected. Date of Collection: Start and end date of data collection.","title":"Dataset Characteristics"},{"location":"template-data-documentation/#data-origin-and-source","text":"Source(s): Provide information about where the data was sourced from (e.g.,public datasets, sensors, surveys, web scraping, crowdsourced). Third-Party Data: Indicate if any part of the dataset was obtained from third parties, and if so, detail the legal agreements in place (license, usage rights, etc.). Ethical Sourcing: Provide information on the ethical and legal compliance of the data collection process (e.g., informed consent, transparency to data subjects, and compliance with GDPR or other regulations).","title":"Data Origin and Source"},{"location":"template-data-documentation/#provenance","text":"Describe the history and origin of the data.","title":"Provenance"},{"location":"template-data-documentation/#collection","text":"","title":"Collection"},{"location":"template-data-documentation/#methods-used","text":"Specify one or more of: API Artificially generated Crowdsourced - Internal Employee Crowdsourced - External Paid Crowdsourced - Volunteer Vendor collection efforts Scraped or crawled Survey, forms, or polls Interviews, focus groups Scientific experiment Taken from other existing datasets Unknown To be determined Others (please specify)","title":"Method(s) Used"},{"location":"template-data-documentation/#methodology-details","text":"EU AI Act Article 11 ; Annex IV 2 (a), (b), (d) Collection Type Source: Describe here. Include links where available. Platform: [Platform Name], Describe platform here. Include links where relevant. Is this source considered sensitive or high-risk? [Yes/No] Dates of Collection: [YYYY-MM -- YYYY-MM] Update Frequency for collected data: Select one for this collection type: yearly, quarterly, monthly, on demand, no changes, others, .... Additional Links for this collection: See section on Access, Rention, and Deletion Additional Notes: Add here","title":"Methodology Detail(s)"},{"location":"template-data-documentation/#source-descriptions","text":"Source: Describe here. Include links, data examples, metrics, visualizations where relevant. Source: Describe here. Include links, data examples, metrics, visualizations where relevant. Source: Describe here. Include links, data examples, metrics, visualizations where relevant. Additional Notes: Add here","title":"Source Description(s)"},{"location":"template-data-documentation/#collection-cadence","text":"Static: Data was collected once from single or multiple sources. Streamed: Data is continuously acquired from single or multiple sources. Dynamic: Data is updated regularly from single or multiple sources. Others: Please specify","title":"Collection Cadence"},{"location":"template-data-documentation/#data-pre-processing","text":"EU AI Act Article 11 ; Annex IV paragraph 2 (d, e)","title":"Data Pre-Processing"},{"location":"template-data-documentation/#data-cleaning","text":"Handling missing data: (e.g., removal, imputation method used) Outlier treatment: (e.g., detection and removal technique) Duplicates removal: (Yes/No) Error correction: (Manual/Automated, if applicable)","title":"Data Cleaning"},{"location":"template-data-documentation/#data-transformation","text":"Normalization/Standardization: (Method used, e.g., min-max scaling) Encoding categorical data: (e.g., one-hot encoding, label encoding) Text/tokenization: (Applicable for NLP tasks)","title":"Data Transformation"},{"location":"template-data-documentation/#feature-engineering","text":"Feature selection: (e.g., methods used to select features) Feature extraction: (e.g., PCA, interaction terms) Newly created features: (List any)","title":"Feature Engineering"},{"location":"template-data-documentation/#dimensionality-reduction","text":"Technique(s) used: (e.g., PCA, t-SNE) Number of dimensions after reduction: (Specify)","title":"Dimensionality Reduction"},{"location":"template-data-documentation/#data-augmentation","text":"Augmentation technique(s): (e.g., rotation, flipping for images)","title":"Data Augmentation"},{"location":"template-data-documentation/#data-annotation-and-labeling","text":"EU AI Act Article 11 ; Annex IV 2(d) Annotation Process: Describe the process used to label or annotate the data (e.g., human labelers, automated, crowdsourcing). Annotation platform Validation: Explain any quality control mechanisms applied to ensure accurate labeling or annotation Inter-Annotator agreement Consensus process Calibration rounds Annotator Demographics (Location / Language / Expertise / Background)","title":"Data Annotation and Labeling"},{"location":"template-data-documentation/#validation-types","text":"","title":"Validation Types"},{"location":"template-data-documentation/#methods","text":"Example= range and constraint validation, structured validation, consistency validation","title":"Method(s)"},{"location":"template-data-documentation/#breakdowns","text":"(Validation Type) Number of Data Points Validated:","title":"Breakdown(s)"},{"location":"template-data-documentation/#descriptions","text":"","title":"Description(s)"},{"location":"template-data-documentation/#sampling-methods","text":"","title":"Sampling Methods"},{"location":"template-data-documentation/#methods-used_1","text":"","title":"Method(s) Used"},{"location":"template-data-documentation/#characteristics","text":"","title":"Characteristic(s)"},{"location":"template-data-documentation/#sampling-criteria","text":"","title":"Sampling Criteria"},{"location":"template-data-documentation/#descriptions_1","text":"","title":"Description(s)"},{"location":"template-data-documentation/#dataset-distribution-and-licensing","text":"EU AI Act Article 11 ; Annex IV 2(d) Availability: Open/public or private dataset Dataset Documentation Link: (Link to further details if available) User Rights and Limitations:","title":"Dataset Distribution and Licensing"},{"location":"template-data-documentation/#access-retention-and-deletion","text":"","title":"Access, Retention, and Deletion"},{"location":"template-data-documentation/#access","text":"","title":"Access"},{"location":"template-data-documentation/#relevant-links_1","text":"[Link to filestore] [Link to governance processes for data access] ...","title":"Relevant Links"},{"location":"template-data-documentation/#data-security-classification-in-and-out-of-scope-delineation","text":"","title":"Data Security Classification in and out of scope delineation"},{"location":"template-data-documentation/#prerequisites","text":"For example: This dataset requires membership in [specific] database groups: Complete the [Mandatory Training] Read [Data Usage Policy] Initiate a [Data Processing Request]","title":"Prerequisite(s)"},{"location":"template-data-documentation/#retention","text":"","title":"Retention"},{"location":"template-data-documentation/#duration","text":"Specify duration in days, months, or years.","title":"Duration"},{"location":"template-data-documentation/#reasons-for-duration","text":"...","title":"Reasons for Duration"},{"location":"template-data-documentation/#policy-summary","text":"Policy: Add a link to the policy if it's standardized at your company","title":"Policy Summary"},{"location":"template-data-documentation/#data-risk-assessment","text":"Describe the assessment of data risks : Foreseeable unintended outcomes or biases arising from dataset use. Sources of potential discrimination or harm.","title":"Data Risk Assessment"},{"location":"template-data-documentation/#cybersecurity-measures","text":"EU AI Act Article 11 ; Annex IV paragraph 5","title":"Cybersecurity Measures"},{"location":"template-data-documentation/#data-security-measures","text":"","title":"Data Security Measures"},{"location":"template-data-documentation/#data-storage","text":"Encryption : Use AES-256; detail key management (e.g., HSM, key rotation). Access Control : Implement role-based access and MFA. Backup : Document backup frequency, encryption, and recovery testing. Integrity Monitoring : Use hashes, checksums, or blockchain. Security : Describe server protections (e.g., restricted access).","title":"Data Storage"},{"location":"template-data-documentation/#data-transfer","text":"Encryption in Transit : Specify TLS 1.3, IPsec configurations. Endpoint Security : Detail device verification and certificate pinning. API Security : Document authentication, rate-limiting, and channel encryption. Data Masking : Use pseudonymisation for sensitive data in transit.","title":"Data Transfer"},{"location":"template-data-documentation/#data-processing","text":"Secure Environments : Use containers, VMs, or trusted execution (e.g., Intel SGX). Audit Logs : Specify logging standards, retention, and tamper protection. Data Minimisation : Anonymise or limit collected data.","title":"Data Processing"},{"location":"template-data-documentation/#standards-applied","text":"","title":"Standards Applied"},{"location":"template-data-documentation/#data-post-market-monitoring","text":"- Data Drift Detection and Monitoring: Describe here what type of drift was identified (covariate drift, prior probability drift or concept drift) - Audit Logs: Periodically perform manual or semi-automated reviews of data samples and log changes in the data as well as access patterns. Action plans implemented to address identified issues: .","title":"Data post-market monitoring"},{"location":"template-data-documentation/#eu-declaration-of-conformity","text":"EU AI Act Article 47","title":"EU Declaration of conformity"},{"location":"template-data-documentation/#standards-applied_1","text":"","title":"Standards applied"},{"location":"template-data-documentation/#documentation-metadata","text":"","title":"Documentation Metadata"},{"location":"template-data-documentation/#version","text":"","title":"Version"},{"location":"template-data-documentation/#template-version","text":"","title":"Template Version"},{"location":"template-data-documentation/#documentation-authors","text":"Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager)","title":"Documentation Authors"},{"location":"template-model-documentation/","text":"Model Documentation Template \u00b6 EU AI Act Article 11 , paragraph 1 Model Owner : Name and contact information Document Version : Version controlling this document is highly recommended Reviewers : List reviewers Overview \u00b6 EU AI Act Article 11 , paragraph 1 Model Type \u00b6 Model Type: (e.g., Neural Networks, Decision Trees, etc.) Model Description \u00b6 EU AI Act Article 11 ; Annex IV paragraph 1(a) Description Status \u00b6 Status Date: YYYY-MM-DD Status: specify one of : Under Preparation -- The model is still under active development and is not yet ready for use due to active \"dev\" updates. Regularly Updated -- New versions of the model have been or will continue to be made available. Actively Maintained -- No new versions will be made available, but this model will be actively maintained. Limited Maintenance -- The model will not be updated, but any technical issues will be addressed. Deprecated -- This model is obsolete or is no longer being maintained. Relevant Links \u00b6 Example references: GitHub Repository Paper/Documentation Link Initiative Demo Conference Talk API Link Developers \u00b6 Name, Team Name, Team Owner \u00b6 Team Name, Contact Person Version Details and Artifacts \u00b6 EU AI Act Article 11 ; Annex IV paragraph 1(c) Current Model Version: Model Version Release Date: Model Version at last Model Documentation Update: Artifacts: Model weights (e.g. S3 bucket path) Model config Intended and Known Usage \u00b6 Intended Use \u00b6 Description Domain(s) of use \u00b6 Description Specific tasks performed: Instructions for use for deployers : EU AI Act Article 13 ; Annex IV Out Of Scope Uses \u00b6 Provide potential applications and/or use cases for which use of the model is not suitable. Known Applications \u00b6 EU AI Act Article 11 ; Annex IV paragraph 1(f) Application Purpose of Model Usage AI Act Risk Application 1 Foundation model providing customer embeddings for fraud detection scoring High Application 2 Customer embeddings used directly as features for recommendation engine Limited Note, this table may not be exhaustive. Model users and documentation consumers at large are highly encouraged to contribute known usages. Model Architecture \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2(b), 2(c) Info \u2013 AI Act requirements: This section should contain a description of the elements of the model and the processes of its training and development. Article 11(2)(b) requires the design specifications of a system, model selection, and what the system is designed to optimize for, as well as potential trade-offs. Article 11(2)(c) requires a description of the system\u2019s architecture, how software components are built on or feed into each other, and the computational resources needed to develop, train, test, and validate the system. Architecture Description Key components Hyperparameter tuning methodology Training Methodology Training duration Compute resources used Data Collection and Preprocessing \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2(d) Steps Involved : Data collection: Describe how the data was sourced (e.g., databases, APIs, sensors, or publicly available datasets). Data cleaning: Explain techniques used to handle missing values, outliers, or errors. Data transformation: Include any scaling or encoding applied. Data Splitting \u00b6 Subset Definitions : Training set : Validation set : Test set : Splitting Methodology : Describe the approach: Random Sampling : Stratified Sampling : Temporal Splits : Proportions : Example: \"70% training, 20% validation, 10% testing.\" Reproducibility : Mention how many seeds are being used and how many are needed to prove statistical significance Data Shuffling : Shuffle applied: (Yes/No) Model Training Process \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2(c, g), paragraph 3 Details of Processes : Initialisation : Loss Function : Optimiser : Hyperparameters : Model Training and Validation \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2(g) EU AI Act Article 15 Objective: Clarify what the model is supposed to achieve. Problem statement (e.g., classification of X, prediction of Y) Business goals (accuracy, fairness, speed) Metrics selected (e.g., accuracy, precision, recall, F1-score, AUC-ROC, MAE, RMSE) Rationale for each metric (why accuracy? why F1-score?) Model predictions on the validation set evalutaion description. Hyperparameter Tuning : Regularisation : Early Stopping : Model Testing and Evaluation \u00b6 Performance Metrics : Compute metrics on the test set: Accuracy, precision, recall, F1 score for classification. MSE, RMSE, MAE for regression. Confusion Matrix : Generate a confusion matrix to evaluate classification results. ROC Curve and AUC : For binary classifiers, compute the ROC curve and Area Under the Curve (AUC). Feature Importance : Analyse feature contributions (for explainability). Robustness Testing : Test the model on edge cases or adversarial examples. Comparison to Baselines : Compare the model\u2019s performance to a simple baseline (e.g., random guess, mean prediction). Model Bias and Fairness Analysis \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2 (f, g), paragraph 3, 4 Implicit Bias, Measurement Bias, Temporal Bias, Selection Bias, Confounding Bias Bias Detection Methods Used \u00b6 Pre-processing: Resampling, Reweighting,Transformation (data imputation, changing order of data); Relabeling, Blinding In-processing: Transfer learning, Reweighting, Constraint optimization, Adversarial Learning, Regularization, Bandits Post-processing: Transformation, Calibration, Thresholding Results of Bias Testing: Mitigation Measures \u00b6 Fairness adjustments: Introduce fairness criteria (like demographic parity, equal opportunity, or equalized odds) into the model training process. Adversarial Debiasing: Use adversarial networks to remove biased information during training. The main model tries to make accurate predictions, while an adversary network tries to predict sensitive attributes from the model's predictions. Retraining approaches \u00b6 Fairness Regularization: Modify the model's objective function to penalize bias. This introduces regularization terms that discourage the model from making predictions that disproportionately affect certain groups. Fair Representation Learning: Learn latent representations of the input data that remove sensitive attributes, ensuring that downstream models trained on these representations are fair. Post-Processing Techniques \u00b6 Fairness-Aware Recalibration: After the model is trained, adjust decision thresholds separately for different demographic groups to reduce disparities in false positive/false negative rates. Output Perturbation: Introduce randomness or noise to model predictions to make outcomes more equitable across groups. Fairness Impact Statement: Explain trade-offs made to satisfy certain fairness criterias Model Interpretability and Explainability \u00b6 EU AI Act Article 11 ; Annex IV paragraph 2(e) Explainability Techniques Used: Examples: SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations) Post-hoc Explanation Models Feature Importance, Permutation Importance, SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations): Partial Dependence Plots (PDP) Counterfactual Explanations Surrogate Models Attention Mechanisms (for Deep Learning) Model-Specific Explanation Techniques Grad-CAM (Gradient-weighted Class Activation Mapping) for CNNs and RNNs: especially for computer vision applications Layer-wise Relevance Propagation (LRP): Works well for CNNs, fully connected nets, and some RNNs (classification focused) TreeSHAP (SHAP for Decision Trees) How interpretable is the model\u2019s decision-making process? EU Declaration of conformity \u00b6 EU AI Act Article 47 (d) Standards applied \u00b6 Documentation Metadata \u00b6 Version \u00b6 Template Version \u00b6 Documentation Authors \u00b6 Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager)","title":"Model Documentation Template"},{"location":"template-model-documentation/#model-documentation-template","text":"EU AI Act Article 11 , paragraph 1 Model Owner : Name and contact information Document Version : Version controlling this document is highly recommended Reviewers : List reviewers","title":"Model Documentation Template"},{"location":"template-model-documentation/#overview","text":"EU AI Act Article 11 , paragraph 1","title":"Overview"},{"location":"template-model-documentation/#model-type","text":"Model Type: (e.g., Neural Networks, Decision Trees, etc.)","title":"Model Type"},{"location":"template-model-documentation/#model-description","text":"EU AI Act Article 11 ; Annex IV paragraph 1(a) Description","title":"Model Description"},{"location":"template-model-documentation/#status","text":"Status Date: YYYY-MM-DD Status: specify one of : Under Preparation -- The model is still under active development and is not yet ready for use due to active \"dev\" updates. Regularly Updated -- New versions of the model have been or will continue to be made available. Actively Maintained -- No new versions will be made available, but this model will be actively maintained. Limited Maintenance -- The model will not be updated, but any technical issues will be addressed. Deprecated -- This model is obsolete or is no longer being maintained.","title":"Status"},{"location":"template-model-documentation/#relevant-links","text":"Example references: GitHub Repository Paper/Documentation Link Initiative Demo Conference Talk API Link","title":"Relevant Links"},{"location":"template-model-documentation/#developers","text":"Name, Team Name, Team","title":"Developers"},{"location":"template-model-documentation/#owner","text":"Team Name, Contact Person","title":"Owner"},{"location":"template-model-documentation/#version-details-and-artifacts","text":"EU AI Act Article 11 ; Annex IV paragraph 1(c) Current Model Version: Model Version Release Date: Model Version at last Model Documentation Update: Artifacts: Model weights (e.g. S3 bucket path) Model config","title":"Version Details and Artifacts"},{"location":"template-model-documentation/#intended-and-known-usage","text":"","title":"Intended and Known Usage"},{"location":"template-model-documentation/#intended-use","text":"Description","title":"Intended Use"},{"location":"template-model-documentation/#domains-of-use","text":"Description Specific tasks performed: Instructions for use for deployers : EU AI Act Article 13 ; Annex IV","title":"Domain(s) of use"},{"location":"template-model-documentation/#out-of-scope-uses","text":"Provide potential applications and/or use cases for which use of the model is not suitable.","title":"Out Of Scope Uses"},{"location":"template-model-documentation/#known-applications","text":"EU AI Act Article 11 ; Annex IV paragraph 1(f) Application Purpose of Model Usage AI Act Risk Application 1 Foundation model providing customer embeddings for fraud detection scoring High Application 2 Customer embeddings used directly as features for recommendation engine Limited Note, this table may not be exhaustive. Model users and documentation consumers at large are highly encouraged to contribute known usages.","title":"Known Applications"},{"location":"template-model-documentation/#model-architecture","text":"EU AI Act Article 11 ; Annex IV paragraph 2(b), 2(c) Info \u2013 AI Act requirements: This section should contain a description of the elements of the model and the processes of its training and development. Article 11(2)(b) requires the design specifications of a system, model selection, and what the system is designed to optimize for, as well as potential trade-offs. Article 11(2)(c) requires a description of the system\u2019s architecture, how software components are built on or feed into each other, and the computational resources needed to develop, train, test, and validate the system. Architecture Description Key components Hyperparameter tuning methodology Training Methodology Training duration Compute resources used","title":"Model Architecture"},{"location":"template-model-documentation/#data-collection-and-preprocessing","text":"EU AI Act Article 11 ; Annex IV paragraph 2(d) Steps Involved : Data collection: Describe how the data was sourced (e.g., databases, APIs, sensors, or publicly available datasets). Data cleaning: Explain techniques used to handle missing values, outliers, or errors. Data transformation: Include any scaling or encoding applied.","title":"Data Collection and Preprocessing"},{"location":"template-model-documentation/#data-splitting","text":"Subset Definitions : Training set : Validation set : Test set : Splitting Methodology : Describe the approach: Random Sampling : Stratified Sampling : Temporal Splits : Proportions : Example: \"70% training, 20% validation, 10% testing.\" Reproducibility : Mention how many seeds are being used and how many are needed to prove statistical significance Data Shuffling : Shuffle applied: (Yes/No)","title":"Data Splitting"},{"location":"template-model-documentation/#model-training-process","text":"EU AI Act Article 11 ; Annex IV paragraph 2(c, g), paragraph 3 Details of Processes : Initialisation : Loss Function : Optimiser : Hyperparameters :","title":"Model Training Process"},{"location":"template-model-documentation/#model-training-and-validation","text":"EU AI Act Article 11 ; Annex IV paragraph 2(g) EU AI Act Article 15 Objective: Clarify what the model is supposed to achieve. Problem statement (e.g., classification of X, prediction of Y) Business goals (accuracy, fairness, speed) Metrics selected (e.g., accuracy, precision, recall, F1-score, AUC-ROC, MAE, RMSE) Rationale for each metric (why accuracy? why F1-score?) Model predictions on the validation set evalutaion description. Hyperparameter Tuning : Regularisation : Early Stopping :","title":"Model Training and Validation"},{"location":"template-model-documentation/#model-testing-and-evaluation","text":"Performance Metrics : Compute metrics on the test set: Accuracy, precision, recall, F1 score for classification. MSE, RMSE, MAE for regression. Confusion Matrix : Generate a confusion matrix to evaluate classification results. ROC Curve and AUC : For binary classifiers, compute the ROC curve and Area Under the Curve (AUC). Feature Importance : Analyse feature contributions (for explainability). Robustness Testing : Test the model on edge cases or adversarial examples. Comparison to Baselines : Compare the model\u2019s performance to a simple baseline (e.g., random guess, mean prediction).","title":"Model Testing and Evaluation"},{"location":"template-model-documentation/#model-bias-and-fairness-analysis","text":"EU AI Act Article 11 ; Annex IV paragraph 2 (f, g), paragraph 3, 4 Implicit Bias, Measurement Bias, Temporal Bias, Selection Bias, Confounding Bias","title":"Model Bias and Fairness Analysis"},{"location":"template-model-documentation/#bias-detection-methods-used","text":"Pre-processing: Resampling, Reweighting,Transformation (data imputation, changing order of data); Relabeling, Blinding In-processing: Transfer learning, Reweighting, Constraint optimization, Adversarial Learning, Regularization, Bandits Post-processing: Transformation, Calibration, Thresholding Results of Bias Testing:","title":"Bias Detection Methods Used"},{"location":"template-model-documentation/#mitigation-measures","text":"Fairness adjustments: Introduce fairness criteria (like demographic parity, equal opportunity, or equalized odds) into the model training process. Adversarial Debiasing: Use adversarial networks to remove biased information during training. The main model tries to make accurate predictions, while an adversary network tries to predict sensitive attributes from the model's predictions.","title":"Mitigation Measures"},{"location":"template-model-documentation/#retraining-approaches","text":"Fairness Regularization: Modify the model's objective function to penalize bias. This introduces regularization terms that discourage the model from making predictions that disproportionately affect certain groups. Fair Representation Learning: Learn latent representations of the input data that remove sensitive attributes, ensuring that downstream models trained on these representations are fair.","title":"Retraining approaches"},{"location":"template-model-documentation/#post-processing-techniques","text":"Fairness-Aware Recalibration: After the model is trained, adjust decision thresholds separately for different demographic groups to reduce disparities in false positive/false negative rates. Output Perturbation: Introduce randomness or noise to model predictions to make outcomes more equitable across groups. Fairness Impact Statement: Explain trade-offs made to satisfy certain fairness criterias","title":"Post-Processing Techniques"},{"location":"template-model-documentation/#model-interpretability-and-explainability","text":"EU AI Act Article 11 ; Annex IV paragraph 2(e) Explainability Techniques Used: Examples: SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations) Post-hoc Explanation Models Feature Importance, Permutation Importance, SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations): Partial Dependence Plots (PDP) Counterfactual Explanations Surrogate Models Attention Mechanisms (for Deep Learning) Model-Specific Explanation Techniques Grad-CAM (Gradient-weighted Class Activation Mapping) for CNNs and RNNs: especially for computer vision applications Layer-wise Relevance Propagation (LRP): Works well for CNNs, fully connected nets, and some RNNs (classification focused) TreeSHAP (SHAP for Decision Trees) How interpretable is the model\u2019s decision-making process?","title":"Model Interpretability and Explainability"},{"location":"template-model-documentation/#eu-declaration-of-conformity","text":"EU AI Act Article 47 (d)","title":"EU Declaration of conformity"},{"location":"template-model-documentation/#standards-applied","text":"","title":"Standards applied"},{"location":"template-model-documentation/#documentation-metadata","text":"","title":"Documentation Metadata"},{"location":"template-model-documentation/#version","text":"","title":"Version"},{"location":"template-model-documentation/#template-version","text":"","title":"Template Version"},{"location":"template-model-documentation/#documentation-authors","text":"Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager) Name, Team: (Owner / Contributor / Manager)","title":"Documentation Authors"}]}